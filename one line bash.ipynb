{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "使用awk从fastq文件中获取序列长度分布\n",
    "\n",
    "zcat file.fastq.gz | awk 'NR%4 == 2 {lengths[length($0)]++} END {for (l in lengths) {print l, lengths[l]}}'  \n",
    "反向互补序列(我使用的时候需要设计引物)\n",
    "\n",
    "echo 'ATTGCTATGCTNNNT' | rev | tr 'ACTG' 'TGAC'\n",
    "使用csplit将多个文件分割成单个文件：\n",
    "\n",
    "csplit -z -q -n 4 -f sequence_ sequences.fasta /\\>/ {*}  \n",
    "通过awk将多FASTA文件分割成单个FASTA文件\n",
    "\n",
    "awk '/^>/{s=++d\".fa\"} {print > s}' multi.fa\n",
    "线性化多线性\n",
    "\n",
    "cat file.fasta | awk '/^>/{if(N>0) printf(\"\\n\"); ++N; printf(\"%s\\t\",$0);next;} {printf(\"%s\",$0);}END{printf(\"\\n\");}'\n",
    "awk 'BEGIN{RS=\">\"}NR>1{sub(\"\\n\",\"\\t\"); gsub(\"\\n\",\"\"); print RS$0}' file.fa\n",
    "fastq2fasta\n",
    "\n",
    "zcat file.fastq.gz | paste - - - - | perl -ane 'print \">$F[0]\\n$F[2]\\n\";' | gzip -c > file.fasta.gz\n",
    "bam2bed\n",
    "\n",
    "samtools view file.bam | perl -F'\\t' -ane '$strand=($F[1]&16)?\"-\":\"+\";$length=1;$tmp=$F[5];$tmp =~ s/(\\d+)[MD]/$length+=$1/eg;print \"$F[2]\\t$F[3]\\t\".($F[3]+$length).\"\\t$F[0]\\t0\\t$strand\\n\";' > file.bed\n",
    "bam2wig\n",
    "\n",
    "samtools mpileup -BQ0 file.sorted.bam | perl -pe '($c, $start, undef, $depth) = split;if ($c ne $lastC || $start != $lastStart+1) {print \"fixedStep chrom=$c start=$start step=1 span=1\\n\";}$_ = $depth.\"\\n\";($lastC, $lastStart) = ($c, $start);' | gzip -c > file.wig.gz\n",
    "Number of reads in a fastq file\n",
    "\n",
    "cat file.fq | echo $((`wc -l`/4))\n",
    "Single line fasta file to multi-line fasta of 60 characteres each line\n",
    "\n",
    "awk -v FS= '/^>/{print;next}{for (i=0;i<=NF/60;i++) {for (j=1;j<=60;j++) printf \"%s\", $(i*60 +j); print \"\"}}' file\n",
    "\n",
    "fold -w 60 file\n",
    "Sequence length of every entry in a multifasta file\n",
    "\n",
    "awk '/^>/ {if (seqlen){print seqlen}; print ;seqlen=0;next; } { seqlen = seqlen +length($0)}END{print seqlen}' file.fa\n",
    "Reproducible subsampling of a FASTQ file. srand() is the seed for the random number generator - keeps the subsampling the same when the script is run multiple times. 0.01 is the % of reads to output.\n",
    "\n",
    "cat file.fq | paste - - - - | awk 'BEGIN{srand(1234)}{if(rand() < 0.01) print $0}' | tr '\\t' '\\n' > out.fq\n",
    "or look at the Hengli's Seqtk\n",
    "\n",
    "Deinterleaving a FASTQ:\n",
    "\n",
    "cat file.fq | paste - - - - - - - - | tee >(cut -f1-4 | tr '\\t'  \n",
    "'\\n' > out1.fq) | cut -f5-8 | tr '\\t' '\\n' > out2.fq\n",
    "Using mpileup for a whole genome can take forever. So, handling each chromosome separately and parallely running them on several cores will speed up your pipeline. Using xargs you can easily realize it.\n",
    "\n",
    "Example usage of xargs (-P is the number of parallel processes started - don't use more than the number of cores you have available):\n",
    "\n",
    "samtools view -H yourFile.bam | grep \"\\@SQ\" | sed 's/^.*SN://g' | cut -f 1 | xargs -I {} -n 1 -P 24 sh -c \"samtools mpileup -BQ0 -d 100000 -uf yourGenome.fa -r {} yourFile.bam | bcftools view -vcg - > tmp.{}.vcf\"\n",
    "To merge the results afterwards, you might want to do something like this:\n",
    "\n",
    "samtools view -H yourFile.bam | grep \"\\@SQ\" | sed 's/^.*SN://g' | cut -f 1 | perl -ane 'system(\"cat tmp.$F[0].bcf >> yourFile.vcf\");'\n",
    "split large file by id/label/column\n",
    "\n",
    "awk '{print >> $1; close($1)}' input_file\n",
    "split a bed file by chromosome:\n",
    "\n",
    "cat nexterarapidcapture_exome_targetedregions_v1.2.bed | sort -k1,1 -k2,2n | sed 's/^chr//' | awk '{close(f);f=$1}{print > f\".bed\"}'\n",
    "\n",
    "#or\n",
    "awk '{print $0 >> $1\".bed\"}' example.bed\n",
    "sort vcf file with header\n",
    "\n",
    "cat my.vcf | awk '$0~\"^#\" { print $0; next } { print $0 | \"sort -k1,1V -k2,2n\" }'\n",
    "Rename a file, bash string manipulation\n",
    "\n",
    "for file in *gz\n",
    "do zcat $file > ${file/bed.gz/bed}\n",
    "gnu sed print invisible characters\n",
    "\n",
    "cat my_file | sed -n 'l'\n",
    "cat -A\n",
    "exit a dead ssh session\n",
    "\n",
    "~.\n",
    "\n",
    "copy large files, copy the from_dir directory inside the to_dir directory\n",
    "\n",
    "rsync -av from_dir  to_dir\n",
    "\n",
    "## copy every file inside the frm_dir to to_dir\n",
    "rsync -av from_dir/ to_dir\n",
    "\n",
    "##re-copy the files avoiding completed ones:\n",
    "\n",
    "rsync -avhP /from/dir /to/dir\n",
    "make directory using the current date\n",
    "\n",
    "mkdir $(date +%F)\n",
    "all the folders' size in the current folder (GNU du)\n",
    "\n",
    "du -h --max-depth=1\n",
    "this one is a bit different, try it and see the difference\n",
    "\n",
    "du -ch\n",
    "\n",
    "the total size of current directory\n",
    "\n",
    "du -sh .\n",
    "\n",
    "disk usage\n",
    "\n",
    "df -h\n",
    "\n",
    "the column names of the file, install csvkit https://csvkit.readthedocs.org/en/0.9.1/\n",
    "\n",
    "csvcut -n\n",
    "\n",
    "open top with human readable size in Mb, Gb. install htop for better visualization\n",
    "\n",
    "top -M\n",
    "\n",
    "how many memeory are used in Gb\n",
    "\n",
    "free -mg\n",
    "\n",
    "print out unique rows based on the first and second column\n",
    "\n",
    "awk '!a[$1,$2]++' input_file\n",
    "\n",
    "sort -u -k1,2 file It will sort based on unique first and second column\n",
    "\n",
    "do not wrap the lines using less\n",
    "\n",
    "less -S\n",
    "\n",
    "pretty output\n",
    "\n",
    "fold -w 60\n",
    "cat file.txt | column -t | less -S\n",
    "pass tab as delimiter http://unix.stackexchange.com/questions/46910/is-it-a-bug-for-join-with-t-t\n",
    "\n",
    "-t $'\\t'\n",
    "\n",
    "awk with the first line printed always\n",
    "\n",
    "awk ' NR ==1 || ($10 > 1 && $11 > 0 && $18 > 0.001)' input_file\n",
    "\n",
    "delete blank lines with sed\n",
    "\n",
    "sed /^$/d\n",
    "\n",
    "delete the last line\n",
    "\n",
    "sed $d\n",
    "\n",
    "awk to join files based on several columns\n",
    "\n",
    "my github repo\n",
    "\n",
    "### select lines from a file based on columns in another file\n",
    "## http://unix.stackexchange.com/questions/134829/compare-two-columns-of-different-files-and-print-if-it-matches\n",
    "awk -F\"\\t\" 'NR==FNR{a[$1$2$3]++;next};a[$1$2$3] > 0' file2 file1 \n",
    "\n",
    "Finally learned about the !$ in unix: take the last thing (word) from the previous command.\n",
    "echo hello, world; echo !$ gives 'world'\n",
    "\n",
    "Create a script of the last executed command:\n",
    "echo \"!!\" > foo.sh\n",
    "\n",
    "Reuse all parameter of the previous command line:\n",
    "!*\n",
    "\n",
    "find bam in current folder (search recursively) and copy it to a new directory using 5 CPUs\n",
    "find . -name \"*bam\" | xargs -P5 -I{} rsync -av {} dest_dir\n",
    "\n",
    "ls -X will group files by extension.\n",
    "\n",
    "loop through all the chromosomes\n",
    "\n",
    "for i in {1..22} X Y \n",
    "do\n",
    "  echo $i\n",
    "done\n",
    "for i in in {01..22} will expand to 01 02 ...\n",
    "\n",
    "change every other newline to tab:\n",
    "\n",
    "paste is used to concatenate corresponding lines from files: paste file1 file2 file3 .... If one of the \"file\" arguments is \"-\", then lines are read from standard input. If there are 2 \"-\" arguments, then paste takes 2 lines from stdin. And so on.\n",
    "\n",
    "cat test.txt  \n",
    "0    ATTTTATTNGAAATAGTAGTGGG\n",
    "0    CTCCCAAAATACTAAAATTATAA\n",
    "1    TTTTAGTTATTTANGAGGTTGAG\n",
    "1    CNTAATCTTAACTCACTACAACC\n",
    "2    TTATAATTTTAGTATTTTGGGAG\n",
    "2    CATATTAACCAAACTAATCTTAA\n",
    "3    GGTTAATATGGTGAAATTTAAT\n",
    "3    ACCTCAACCTCNTAAATAACTAA\n",
    "\n",
    "cat test.txt| paste - -                               \n",
    "0    ATTTTATTNGAAATAGTAGTGGG    0    CTCCCAAAATACTAAAATTATAA\n",
    "1    TTTTAGTTATTTANGAGGTTGAG    1    CNTAATCTTAACTCACTACAACC\n",
    "2    TTATAATTTTAGTATTTTGGGAG    2    CATATTAACCAAACTAATCTTAA\n",
    "3    GGTTAATATGGTGAAATTTAAT     3    ACCTCAACCTCNTAAATAACTAA\n",
    "ORS: output record seperator in awk var=condition?condition_if_true:condition_if_false is the ternary operator.\n",
    "\n",
    "cat test.txt| awk 'ORS=NR%2?\"\\t\":\"\\n\"'          \n",
    "\n",
    "0    ATTTTATTNGAAATAGTAGTGGG    0    CTCCCAAAATACTAAAATTATAA\n",
    "1    TTTTAGTTATTTANGAGGTTGAG    1    CNTAATCTTAACTCACTACAACC\n",
    "2    TTATAATTTTAGTATTTTGGGAG    2    CATATTAACCAAACTAATCTTAA\n",
    "3    GGTTAATATGGTGAAATTTAAT     3    ACCTCAACCTCNTAAATAACTAA\n",
    "awk\n",
    "\n",
    "We can also use the concept of a conditional operator in print statement of the form print CONDITION ? PRINT_IF_TRUE_TEXT : PRINT_IF_FALSE_TEXT. For example, in the code below, we identify sequences with lengths > 14:\n",
    "\n",
    "cat data/test.tsv\n",
    "blah_C1\tACTGTCTGTCACTGTGTTGTGATGTTGTGTGTG\n",
    "blah_C2\tACTTTATATATT\n",
    "blah_C3\tACTTATATATATATA\n",
    "blah_C4\tACTTATATATATATA\n",
    "blah_C5\tACTTTATATATT\t\n",
    "\n",
    "awk '{print (length($2)>14) ? $0\">14\" : $0\"<=14\";}' data/test.tsv\n",
    "blah_C1\tACTGTCTGTCACTGTGTTGTGATGTTGTGTGTG>14\n",
    "blah_C2\tACTTTATATATT<=14\n",
    "blah_C3\tACTTATATATATATA>14\n",
    "blah_C4\tACTTATATATATATA>14\n",
    "blah_C5\tACTTTATATATT<=14\n",
    "\n",
    "awk 'NR==3{print \"\";next}{printf $1\"\\t\"}{print $1}' data/test.tsv\n",
    "blah_C1\tblah_C1\n",
    "blah_C2\tblah_C2\n",
    "\n",
    "blah_C4\tblah_C4\n",
    "blah_C5\tblah_C5\n",
    "You can also use getline to load the contents of another file in addition to the one you are reading, for example, in the statement given below, the while loop will load each line from test.tsv into k until no more lines are to be read:\n",
    "\n",
    "awk 'BEGIN{while((getline k <\"data/test.tsv\")>0) print \"BEGIN:\"k}{print}' data/test.tsv\n",
    "BEGIN:blah_C1\tACTGTCTGTCACTGTGTTGTGATGTTGTGTGTG\n",
    "BEGIN:blah_C2\tACTTTATATATT\n",
    "BEGIN:blah_C3\tACTTATATATATATA\n",
    "BEGIN:blah_C4\tACTTATATATATATA\n",
    "BEGIN:blah_C5\tACTTTATATATT\n",
    "blah_C1\tACTGTCTGTCACTGTGTTGTGATGTTGTGTGTG\n",
    "blah_C2\tACTTTATATATT\n",
    "blah_C3\tACTTATATATATATA\n",
    "blah_C4\tACTTATATATATATA\n",
    "blah_C5\tACTTTATATATT\n",
    "merge multiple fasta sequences in two files into a single file line by line\n",
    "\n",
    "see post\n",
    "\n",
    "linearize.awk:\n",
    "\n",
    "/^>/ {printf(\"%s%s\\t\",(N>0?\"\\n\":\"\"),$0);N++;next;} {printf(\"%s\",$0);} END {printf(\"\\n\");}\n",
    "paste <(awk -f linearize.awk file1.fa ) <(awk -f linearize.awk file2.fa  )| tr \"\\t\" \"\\n\"\n",
    "grep fastq reads containing a pattern but maintain the fastq format\n",
    "\n",
    "grep -A 2 -B 1 'AAGTTGATAACGGACTAGCCTTATTTT' file.fq | sed '/^--$/d' > out.fq\n",
    "\n",
    "# or\n",
    "zcat reads.fq.gz \\\n",
    "| paste - - - - \\\n",
    "| awk -v FS=\"\\t\" -v OFS=\"\\n\" '$2 ~ \"AAGTTGATAACGGACTAGCCTTATTTT\" {print $1, $2, $3, $4}' \\\n",
    "| gzip > filtered.fq.gz\n",
    "count how many columns of a tsv files:\n",
    "\n",
    "cat file.tsv | head -1 | tr \"\\t\" \"\\n\" | wc -l  \n",
    "csvcut -n -t  file.tsv (from csvkit)\n",
    "awk '{print NF; exit}' file.tsv\n",
    "awk -F \"\\t\" 'NR == 1 {print NF}' file.tsv\n",
    "combine info to the fasta header\n",
    "\n",
    "from biostar post\n",
    "\n",
    "cat myfasta.txt \n",
    ">Blap_contig79\n",
    "MSTDVDAKTRSKERASIAAFYVGRNIFVTGGTGFLGKVLIEKLLRSCPDVGEIFILMRPKAGLSI\n",
    ">Bluc_contig23663\n",
    "MSTNVDAKARSKERASIAAFYVGRNIFVTGGTGFLGKVLIEKLLRSCPDVGEIFILMRPKAGLSI\n",
    ">Blap_contig7988\n",
    "MSTDVDAKTRSKERASIAAFYVGRNIFVTGGTGFLGKVLIEKLLRSCPDVGEIFILMRPKAGLSI\n",
    ">Bluc_contig1223663\n",
    "MSTNVDAKARSKERASIAAFYVGRNIFVTGGTGFLGKVLIEKLLRSCPDVGEIFILMRPKAGLSI\n",
    "\n",
    "cat my_info.txt \n",
    "info1\n",
    "info2\n",
    "info3\n",
    "info4\n",
    "\n",
    "paste <(cat my_info.txt) <(cat myfasta.txt| paste - - | cut -c2-) | awk '{printf(\">%s_%s\\n%s\\n\",$1,$2,$3);}'\n",
    ">info1_Blap_contig79\n",
    "MSTDVDAKTRSKERASIAAFYVGRNIFVTGGTGFLGKVLIEKLLRSCPDVGEIFILMRPKAGLSI\n",
    ">info2_Bluc_contig23663\n",
    "MSTNVDAKARSKERASIAAFYVGRNIFVTGGTGFLGKVLIEKLLRSCPDVGEIFILMRPKAGLSI\n",
    ">info3_Blap_contig7988\n",
    "MSTDVDAKTRSKERASIAAFYVGRNIFVTGGTGFLGKVLIEKLLRSCPDVGEIFILMRPKAGLSI\n",
    ">info4_Bluc_contig1223663\n",
    "MSTNVDAKARSKERASIAAFYVGRNIFVTGGTGFLGKVLIEKLLRSCPDVGEIFILMRPKAGLSI\n",
    "count how many columns in a tsv file\n",
    "\n",
    "cat file.tsv | head -1 | tr \"\\t\" \"\\n\" | wc -l  \n",
    "\n",
    "##(from csvkit)\n",
    "csvcut -n -t file.\n",
    "\n",
    "## emulate csvcut -n -t\n",
    "less files.tsv | head -1| tr \"\\t\" \"\\n\" | nl\n",
    "\n",
    "awk -F \"\\t\" 'NR == 1 {print NF}' file.tsv\n",
    "awk '{print NF; exit}'\n",
    "change fasta header\n",
    "\n",
    "see https://www.biostars.org/p/53212/\n",
    "\n",
    "The fasta header is like >7 dna:chromosome chromosome:GRCh37:7:1:159138663:1 convert to >7:\n",
    "\n",
    "cat Homo_sapiens_assembly19.fasta | gawk '/^>/ { b=gensub(\" dna:.+\", \"\", \"g\", $0); print b; next} {print}' > Homo_sapiens_assembly19_reheader.fasta\n",
    "mkdir and cd into that dir shortcut\n",
    "\n",
    "mkdir blah && cd $_\n",
    "cut out columns based on column names in another file\n",
    "\n",
    "http://crazyhottommy.blogspot.com/2016/10/cutting-out-500-columns-from-26g-file.html\n",
    "\n",
    "#! /bin/bash\n",
    "\n",
    "set -e\n",
    "set -u\n",
    "set -o pipefail\n",
    "\n",
    "#### Author: Ming Tang (Tommy)\n",
    "#### Date 09/29/2016\n",
    "#### I got the idea from this stackOverflow post http://stackoverflow.com/questions/11098189/awk-extract-columns-from-file-based-on-header-selected-from-2nd-file\n",
    "\n",
    "# show help\n",
    "show_help(){\n",
    "cat << EOF\n",
    "  This is a wrapper extracting columns of a (big) dataframe based on a list of column names in another\n",
    "  file. The column names must be one per line. The output will be stdout. For small files < 2G, one \n",
    "  can load it into R and do it easily, but when the file is big > 10G. R is quite cubersome. \n",
    "  Using unix commands on the other hand is better because files do not have to be loaded into memory at once.\n",
    "  e.g. subset a 26G size file for 700 columns takes around 30 mins. Memory footage is very low ~4MB.\n",
    "\n",
    "  usage: ${0##*/} -f < a dataframe  > -c < colNames> -d <delimiter of the file>\n",
    "        -h display this help and exit.\n",
    "\t\t-f the file you want to extract columns from. must contain a header with column names.\n",
    "\t\t-c a file with the one column name per line.\n",
    "\t\t-d delimiter of the dataframe: , or \\t. default is tab.  \n",
    "\t\t\n",
    "\t\te.g. \n",
    "\t\t\n",
    "\t\tfor tsv file:\n",
    "\t\t\t${0##*/} -f mydata.tsv -c colnames.txt -d $'\\t' or simply ommit the -d, default is tab.\n",
    "\t\t\n",
    "\t\tfor csv file: Note you have to specify -d , if your file is csv, otherwise all columns will be cut out.\n",
    "\t\t\t${0##*/} -f mydata.csv -c colnames.txt -d ,\n",
    "        \n",
    "EOF\n",
    "}\n",
    "\n",
    "## if there are no arguments provided, show help\n",
    "if [[ $# == 0 ]]; then show_help; exit 1; fi\n",
    "\n",
    "while getopts \":hf:c:d:\" opt; do\n",
    "  case \"$opt\" in\n",
    "    h) show_help;exit 0;;\n",
    "    f) File2extract=$OPTARG;;\n",
    "    c) colNames=$OPTARG;;\n",
    "    d) delim=$OPTARG;;\n",
    "    '?') echo \"Invalid option $OPTARG\"; show_help >&2; exit 1;;\n",
    "  esac\n",
    "done\n",
    "\t\n",
    "\n",
    "## set up the default delimiter to be tab, Note the way I specify tab \n",
    "\n",
    "delim=${delim:-$'\\t'}\n",
    "\n",
    "## get the number of columns in the data frame that match the column names in the colNames file.\n",
    "## change the output to 2,5,6,22,... and get rid of the last comma  so cut -f can be used\n",
    " \n",
    "cols=$(head -1 \"${File2extract}\" | tr \"${delim}\" \"\\n\" | grep -nf \"${colNames}\" | sed 's/:.*$//' | tr \"\\n\" \",\" | sed 's/,$//')\n",
    "\n",
    "## cut out the columns \n",
    "cut -d\"${delim}\" -f\"${cols}\" \"${File2extract}\"\n",
    "or use csvtk from Shen Wei:\n",
    "\n",
    "csvtk cut -t -f $(paste -s -d , list.txt) data.tsv\n",
    "merge all bed files and add a column for the filename.\n",
    "\n",
    "awk '{print $0 \"\\t\" FILENAME}' *bed \n",
    "add or remove chr from the start of each line\n",
    "\n",
    "# add chr\n",
    "sed 's/^/chr/' my.bed\n",
    "\n",
    "# or\n",
    "awk 'BEGIN {OFS = \"\\t\"} {$1=\"chr\"$1; print}'\n",
    "\n",
    "# remove chr\n",
    "sed 's/^chr//' my.bed\n",
    "check if a tsv files have the same number of columns for all rows\n",
    "\n",
    "awk '{print NF}' test.tsv | sort -nu | head -n 1\n",
    "Parallelized samtools mpileup\n",
    "\n",
    "https://www.biostars.org/p/134331/\n",
    "\n",
    "BAM=\"yourFile.bam\"\n",
    "REF=\"reference.fasta\"\n",
    "samtools view -H $BAM | grep \"\\@SQ\" | sed 's/^.*SN://g' | cut -f 1 | xargs -I {} -n 1 -P 24 sh -c \"samtools mpileup -BQ0 -d 100000 -uf $REF -r \\\"{}\\\" $BAM | bcftools call -cv > \\\"{}\\\".vcf\"\n",
    "convert multiple lines to a single line\n",
    "\n",
    "This is better than tr \"\\n\" \"\\t\" because somtimes I do not want to convert the last newline to tab.\n",
    "\n",
    "cat myfile.txt | paste -s \n",
    "merge multiple files with same header by keeping the header of the first file\n",
    "\n",
    "I usually do it in R, but like the quick solution.\n",
    "\n",
    "https://stackoverflow.com/questions/16890582/unixmerge-multiple-csv-files-with-same-header-by-keeping-the-header-of-the-firs\n",
    "\n",
    "awk 'FNR==1 && NR!=1{next;}{print}' *.csv \n",
    "\n",
    "# or\n",
    "\n",
    "awk '\n",
    "    FNR==1 && NR!=1 { while (/^<header>/) getline; }\n",
    "    1 {print}\n",
    "' file*.txt >all.txt\n",
    "insert a field into the first line\n",
    "\n",
    "cut -f1-4 F5.hg38.enhancers.expression.usage.matrix | head\n",
    "CNhs11844\tCNhs11251\tCNhs11282\tCNhs10746\n",
    "chr10:100006233-100006603\t1\t0\t0\n",
    "chr10:100008181-100008444\t0\t0\t0\n",
    "chr10:100014348-100014634\t0\t0\t0\n",
    "chr10:100020065-100020562\t0\t0\t0\n",
    "chr10:100043485-100043744\t0\t0\t0\n",
    "chr10:100114218-100114567\t0\t0\t0\n",
    "chr10:100148595-100148922\t0\t0\t0\n",
    "chr10:100182422-100182522\t0\t0\t0\n",
    "chr10:100184498-100184704\t0\t0\t0\n",
    "\n",
    "sed '1 s/^/enhancer\\t/' F5.hg38.enhancers.expression.usage.matrix | cut -f1-4 | head\n",
    "enhancer\tCNhs11844\tCNhs11251\tCNhs11282\n",
    "chr10:100006233-100006603\t1\t0\t0\n",
    "chr10:100008181-100008444\t0\t0\t0\n",
    "chr10:100014348-100014634\t0\t0\t0\n",
    "chr10:100020065-100020562\t0\t0\t0\n",
    "chr10:100043485-100043744\t0\t0\t0\n",
    "chr10:100114218-100114567\t0\t0\t0\n",
    "chr10:100148595-100148922\t0\t0\t0\n",
    "chr10:100182422-100182522\t0\t0\t0\n",
    "chr10:100184498-100184704\t0\t0\t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Part I: Working With Files\n",
    "\n",
    "1. Empty a file (truncate to 0 size)\n",
    "\n",
    "$ > file\n",
    "\n",
    "This one-liner uses the output redirection operator >. Redirection of output causes the file to be opened for writing. If the file does not exist it is created; if it does exist it is truncated to zero size. As we're not redirecting anything to the file it remains empty.\n",
    "\n",
    "If you wish to replace the contents of a file with some string or create a file with specific content, you can do this:\n",
    "\n",
    "$ echo \"some string\" > file\n",
    "\n",
    "This puts the string \"some string\" in the file.\n",
    "\n",
    "2. Append a string to a file\n",
    "\n",
    "$ echo \"foo bar baz\" >> file\n",
    "\n",
    "This one-liner uses a different output redirection operator >>, which appends to the file. If the file does not exist it is created. The string appended to the file is followed by a newline. If you don't want a newline appended after the string, add the -n argument to echo:\n",
    "\n",
    "$ echo -n \"foo bar baz\" >> file\n",
    "\n",
    "3. Read the first line from a file and put it in a variable\n",
    "\n",
    "$ read -r line < file\n",
    "\n",
    "This one-liner uses the built-in bash command read and the input redirection operator <. The read command reads one line from the standard input and puts it in the line variable. The -r parameter makes sure the input is read raw, meaning the backslashes won't get escaped (they'll be left as is). The redirection command < file opens file for reading and makes it the standard input to the read command.\n",
    "\n",
    "The read command removes all characters present in the special IFS variable. IFS stands for Internal Field Separator that is used for word splitting after expansion and to split lines into words with the read built-in command. By default IFS contains space, tab, and newline, which means that the leading and trailing tabs and spaces will get removed. If you wish to preserve them, you can set IFS to nothing for the time being:\n",
    "\n",
    "$ IFS= read -r line < file\n",
    "\n",
    "This will change the value of IFS just for this command and will make sure the first line gets read into the line variable really raw with all the leading and trailing whitespaces.\n",
    "\n",
    "Another way to read the first line from a file into a variable is to do this:\n",
    "\n",
    "$ line=$(head -1 file)\n",
    "\n",
    "This one-liner uses the command substitution operator $(...). It runs the command in ..., and returns its output. In this case the command is head -1 file that outputs the first line of the file. The output is then assigned to the line variable. Using $(...) is exactly the same as `...`, so you could have also written:\n",
    "\n",
    "$ line=`head -1 file`\n",
    "\n",
    "However $(...) is the preferred way in bash as it's cleaner and easier to nest.\n",
    "\n",
    "4. Read a file line-by-line\n",
    "\n",
    "$ while read -r line; do\n",
    "    # do something with $line\n",
    "done < file\n",
    "\n",
    "This is the one and only right way to read lines from a file one-by-one. This method puts the read command in a while loop. When the read command encounters end-of-file, it returns a positive return code (code for failure) and the while loop stops.\n",
    "\n",
    "Remember that read trims leading and trailing whitespace, so if you wish to preserve it, clear the IFS variable:\n",
    "\n",
    "$ while IFS= read -r line; do\n",
    "    # do something with $line\n",
    "done < file\n",
    "\n",
    "If you don't like the to put < file at the end, you can also pipe the contents of the file to the while loop:\n",
    "\n",
    "$ cat file | while IFS= read -r line; do\n",
    "    # do something with $line\n",
    "done\n",
    "\n",
    "5. Read a random line from a file and put it in a variable\n",
    "\n",
    "$ read -r random_line < <(shuf file)\n",
    "\n",
    "There is no clean way to read a random line from a file with just bash, so we'll need to use some external programs for help. If you're on a modern Linux machine, then it comes with the shuf utility that's in GNU coreutils.\n",
    "\n",
    "This one-liner uses the process substitution <(...) operator. This process substitution operator creates an anonymous named pipe, and connects the stdout of the process to the write part of the named pipe. Then bash executes the process, and it replaces the whole process substitution expression with the filename of the anonymous named pipe.\n",
    "\n",
    "When bash sees <(shuf file) it opens a special file /dev/fd/n, where n is a free file descriptor, then runs shuf file with its stdout connected to /dev/fd/n and replaces <(shuf file) with /dev/fd/n so the command effectively becomes:\n",
    "\n",
    "$ read -r random_line < /dev/fd/n\n",
    "\n",
    "Which reads the first line from the shuffled file.\n",
    "\n",
    "Here is another way to do it with the help of GNU sort. GNU sort takes the -R option that randomizes the input.\n",
    "\n",
    "$ read -r random_line < <(sort -R file)\n",
    "\n",
    "Another way to get a random line in a variable is this:\n",
    "\n",
    "$ random_line=$(sort -R file | head -1)\n",
    "\n",
    "Here the file gets randomly sorted by sort -R and then head -1 takes the first line.\n",
    "\n",
    "6. Read the first three columns/fields from a file into variables\n",
    "\n",
    "$ while read -r field1 field2 field3 throwaway; do\n",
    "    # do something with $field1, $field2, and $field3\n",
    "done < file\n",
    "\n",
    "If you specify more than one variable name to the read command, it shall split the line into fields (splitting is done based on what's in the IFS variable, which contains a whitespace, a tab, and a newline by default), and put the first field in the first variable, the second field in the second variable, etc., and it will put the remaining fields in the last variable. That's why we have the throwaway variable after the three field variables. if we didn't have it, and the file had more than three columns, the third field would also get the leftovers.\n",
    "\n",
    "Sometimes it's shorter to just write _ for the throwaway variable:\n",
    "\n",
    "$ while read -r field1 field2 field3 _; do\n",
    "    # do something with $field1, $field2, and $field3\n",
    "done < file\n",
    "\n",
    "Or if you have a file with exactly three fields, then you don't need it at all:\n",
    "\n",
    "$ while read -r field1 field2 field3; do\n",
    "    # do something with $field1, $field2, and $field3\n",
    "done < file\n",
    "\n",
    "Here is an example. Let's say you wish to find out number of lines, number of words, and number of bytes in a file. If you run wc on a file you get these 3 numbers plus the filename as the fourth field:\n",
    "\n",
    "$ cat file-with-5-lines\n",
    "x 1\n",
    "x 2\n",
    "x 3\n",
    "x 4\n",
    "x 5\n",
    "\n",
    "$ wc file-with-5-lines\n",
    " 5 10 20 file-with-5-lines\n",
    "\n",
    "So this file has 5 lines, 10 words, and 20 chars. We can use the read command to get this info into variables:\n",
    "\n",
    "$ read lines words chars _ < <(wc file-with-5-lines)\n",
    "\n",
    "$ echo $lines\n",
    "5\n",
    "$ echo $words\n",
    "10\n",
    "$ echo $chars\n",
    "20\n",
    "\n",
    "Similarly you can use here-strings to split strings into variables. Let's say you have a string \"20 packets in 10 seconds\" in a $info variable and you want to extract 20 and 10. Not too long ago I'd have written this:\n",
    "\n",
    "$ packets=$(echo $info | awk '{ print $1 }')\n",
    "$ time=$(echo $info | awk '{ print $4 }')\n",
    "\n",
    "However given the power of read and our bash knowledge, we can now do this:\n",
    "\n",
    "$ read packets _ _ time _ <<< \"$info\"\n",
    "\n",
    "Here the <<< is a here-string, which lets you pass strings directly to the standard input of commands.\n",
    "\n",
    "7. Find the size of a file, and put it in a variable\n",
    "\n",
    "$ size=$(wc -c < file)\n",
    "\n",
    "This one-liner uses the command substitution operator $(...) that I explained in one-liner #3. It runs the command in ..., and returns its output. In this case the command is wc -c < file that prints the number of chars (bytes) in the file. The output is then assigned to size variable.\n",
    "\n",
    "8. Extract the filename from the path\n",
    "\n",
    "Let's say you have a /path/to/file.ext, and you wish to extract just the filename file.ext. How do you do it? A good solution is to use the parameter expansion mechanism:\n",
    "\n",
    "$ filename=${path##*/}\n",
    "\n",
    "This one-liner uses the ${var##pattern} parameter expansion. This expansion tries to match the pattern at the beginning of the $var variable. If it matches, then the result of the expansion is the value of $var with the longest matching pattern deleted.\n",
    "\n",
    "In this case the pattern is */ which matches at the beginning of /path/to/file.ext and as it's a greedy match, the pattern matches all the way till the last slash (it matches /path/to/). The result of this expansion is then just the filename file.ext as the matched pattern gets deleted.\n",
    "\n",
    "9. Extract the directory name from the path\n",
    "\n",
    "This is similar to the previous one-liner. Let's say you have a /path/to/file.ext, and you wish to extract just the path to the file /path/to. You can use the parameter expansion again:\n",
    "\n",
    "$ dirname=${path%/*}\n",
    "\n",
    "This time it's the ${var%pattern} parameter expansion that tries to match the pattern at the end of the $var variable. If the pattern matches, then the result of the expansion is the value of $var shortest matching pattern deleted.\n",
    "\n",
    "In this case the pattern is /*, which matches at the end of /path/to/file.ext (it matches /file.ext). The result then is just the dirname /path/to as the matched pattern gets deleted.\n",
    "\n",
    "10. Make a copy of a file quickly\n",
    "\n",
    "Let's say you wish to copy the file at /path/to/file to /path/to/file_copy. Normally you'd write:\n",
    "\n",
    "$ cp /path/to/file /path/to/file_copy\n",
    "\n",
    "However you can do it much quicker by using the brace expansion {...}:\n",
    "\n",
    "$ cp /path/to/file{,_copy}\n",
    "\n",
    "Brace expansion is a mechanism by which arbitrary strings can be generated. In this particular case /path/to/file{,_copy} generates the string /path/to/file /path/to/file_copy, and the whole command becomes cp /path/to/file /path/to/file_copy.\n",
    "\n",
    "Similarly you can move a file quickly:\n",
    "\n",
    "$ mv /path/to/file{,_old}\n",
    "\n",
    "This expands to mv /path/to/file /path/to/file_old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt II: Working With Strings\n",
    "\n",
    "1. Generate the alphabet from a-z\n",
    "\n",
    "$ echo {a..z}\n",
    "\n",
    "This one-liner uses brace expansion. Brace expansion is a mechanism for generating arbitrary strings. This one-liner uses a sequence expression of the form {x..y}, where x and y are single characters. The sequence expression expands to each character lexicographically between x and y, inclusive.\n",
    "\n",
    "If you run it, you get all the letters from a-z:\n",
    "\n",
    "$ echo {a..z}\n",
    "a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
    "\n",
    "2. Generate the alphabet from a-z without spaces between characters\n",
    "\n",
    "$ printf \"%c\" {a..z}\n",
    "\n",
    "This is an awesome bash trick that 99.99% bash users don't know about. If you supply a list of items to the printf function it actually applies the format in a loop until the list is empty! printf as a loop! There is nothing more awesome than that!\n",
    "\n",
    "In this one-liner the printf format is \"%c\", which means \"a character\" and the arguments are all letters from a-z separated by space. So what printf does is it iterates over the list outputting each character after character until it runs out of letters.\n",
    "\n",
    "Here is the output if you run it:\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyz\n",
    "\n",
    "This output is without a terminating newline because the format string was \"%c\" and it doesn't include \\n. To have it newline terminated, just add $'\\n' to the list of chars to print:\n",
    "\n",
    "$ printf \"%c\" {a..z} $'\\n'\n",
    "\n",
    "$'\\n' is bash idiomatic way to represent a newline character. printf then just prints chars a to z, and the newline character.\n",
    "\n",
    "Another way to add a trailing newline character is to echo the output of printf:\n",
    "\n",
    "$ echo $(printf \"%c\" {a..z})\n",
    "\n",
    "This one-liner uses command substitution, which runs printf \"%c\" {a..z} and replaces the command with its output. Then echo prints this output and adds a newline itself.\n",
    "\n",
    "Want to output all letters in a column instead? Add a newline after each character!\n",
    "\n",
    "$ printf \"%c\\n\" {a..z}\n",
    "\n",
    "Output:\n",
    "\n",
    "a\n",
    "b\n",
    "...\n",
    "z\n",
    "\n",
    "Want to put the output from printf in a variable quickly? Use the -v argument:\n",
    "\n",
    "$ printf -v alphabet \"%c\" {a..z}\n",
    "\n",
    "This puts abcdefghijklmnopqrstuvwxyz in the $alphabet variable.\n",
    "\n",
    "Similarly you can generate a list of numbers. Let's say from 1 to 100:\n",
    "\n",
    "$ echo {1..100}\n",
    "\n",
    "Output:\n",
    "\n",
    "1 2 3 ... 100\n",
    "\n",
    "Alternatively, if you forget this method, you can use the external seq utility to generate a sequence of numbers:\n",
    "\n",
    "$ seq 1 100\n",
    "\n",
    "3. Pad numbers 0 to 9 with a leading zero\n",
    "\n",
    "$ printf \"%02d \" {0..9}\n",
    "\n",
    "Here we use the looping abilities of printf again. This time the format is \"%02d \", which means \"zero pad the integer up to two positions\", and the items to loop through are the numbers 0-9, generated by the brace expansion (as explained in the previous one-liner).\n",
    "\n",
    "Output:\n",
    "\n",
    "00 01 02 03 04 05 06 07 08 09\n",
    "\n",
    "If you use bash 4, you can do the same with the new feature of brace expansion:\n",
    "\n",
    "$ echo {00..09}\n",
    "\n",
    "Older bashes don't have this feature.\n",
    "\n",
    "4. Produce 30 English words\n",
    "\n",
    "$ echo {w,t,}h{e{n{,ce{,forth}},re{,in,fore,with{,al}}},ither,at}\n",
    "\n",
    "This is an abuse of brace expansion. Just look at what this produces:\n",
    "\n",
    "when whence whenceforth where wherein wherefore wherewith wherewithal whither what then thence thenceforth there therein therefore therewith therewithal thither that hen hence henceforth here herein herefore herewith herewithal hither hat\n",
    "Crazy awesome!\n",
    "\n",
    "Here is how it works - you can produce permutations of words/symbols with brace expansion. For example, if you do this,\n",
    "\n",
    "$ echo {a,b,c}{1,2,3}\n",
    "\n",
    "It will produce the result a1 a2 a3 b1 b2 b3 c1 c2 c3. It takes the first a, and combines it with {1,2,3}, producing a1 a2 a3. Then it takes b and combines it with {1,2,3}, and then it does the same for c.\n",
    "\n",
    "So this one-liner is just a smart combination of braces that when expanded produce all these English words!\n",
    "\n",
    "5. Produce 10 copies of the same string\n",
    "\n",
    "$ echo foo{,,,,,,,,,,}\n",
    "\n",
    "This one-liner uses the brace expansion again. What happens here is foo gets combined with 10 empty strings, so the output is 10 copies of foo:\n",
    "\n",
    "foo foo foo foo foo foo foo foo foo foo foo\n",
    "6. Join two strings\n",
    "\n",
    "$ echo \"$x$y\"\n",
    "\n",
    "This one-liner simply concatenates two variables together. If the variable x contains foo and y contains bar then the result is foobar.\n",
    "\n",
    "Notice that \"$x$y\" were quoted. If we didn't quote it, echo would interpret the $x$y as regular arguments, and would first try to parse them to see if they contain command line switches. So if $x contains something beginning with -, it would be a command line argument rather than an argument to echo:\n",
    "\n",
    "x=-n\n",
    "y=\" foo\"\n",
    "echo $x$y\n",
    "Output:\n",
    "\n",
    "foo\n",
    "Versus the correct way:\n",
    "\n",
    "x=-n\n",
    "y=\" foo\"\n",
    "echo \"$x$y\"\n",
    "Output:\n",
    "\n",
    "-n foo\n",
    "If you need to put the two joined strings in a variable, you can omit the quotes:\n",
    "\n",
    "var=$x$y\n",
    "7. Split a string on a given character\n",
    "\n",
    "Let's say you have a string foo-bar-baz in the variable $str and you wish to split it on the dash and iterate over it. You can simply combine IFS with read to do it:\n",
    "\n",
    "$ IFS=- read -r x y z <<< \"$str\"\n",
    "\n",
    "Here we use the read x command that reads data from stdin and puts the data in the x y z variables. We set IFS to - as this variable is used for field splitting. If multiple variable names are specified to read, IFS is used to split the line of input so that each variable gets a single field of the input.\n",
    "\n",
    "In this one-liner $x gets foo, $y gets bar, $z gets baz.\n",
    "\n",
    "Also notice the use of <<< operator. This is the here-string operator that allows strings to be passed to stdin of commands easily. In this case string $str is passed as stdin to read.\n",
    "\n",
    "You can also put the split fields and put them in an array:\n",
    "\n",
    "$ IFS=- read -ra parts <<< \"foo-bar-baz\"\n",
    "\n",
    "The -a argument to read makes it put the split words in the given array. In this case the array is parts. You can access array elements through ${parts[0]}, ${parts[1]}, and ${parts[0]}. Or just access all of them through ${parts[@]}.\n",
    "\n",
    "8. Process a string character by character\n",
    "\n",
    "$ while IFS= read -rn1 c; do\n",
    "    # do something with $c\n",
    "done <<< \"$str\"\n",
    "\n",
    "Here we use the -n1 argument to read command to make it read the input character at a time. Similarly we can use -n2 to read two chars at a time, etc.\n",
    "\n",
    "9. Replace \"foo\" with \"bar\" in a string\n",
    "\n",
    "$ echo ${str/foo/bar}\n",
    "\n",
    "This one-liner uses parameter expansion of form ${var/find/replace}. It finds the string find in var and replaces it with replace. Really simple!\n",
    "\n",
    "To replace all occurrences of \"foo\" with \"bar\", use the ${var//find/replace} form:\n",
    "\n",
    "$ echo ${str//foo/bar}\n",
    "\n",
    "10. Check if a string matches a pattern\n",
    "\n",
    "$ if [[ $file = *.zip ]]; then\n",
    "    # do something\n",
    "fi\n",
    "\n",
    "Here the one-liner does something if $file matches *.zip. This is a simple glob pattern matching, and you can use symbols * ? [...] to do matching. Code * matches any string, ? matches a single char, and [...] matches any character in ... or a character class.\n",
    "\n",
    "Here is another example that matches if answer is Y or y:\n",
    "\n",
    "$ if [[ $answer = [Yy]* ]]; then\n",
    "    # do something\n",
    "fi\n",
    "\n",
    "11. Check if a string matches a regular expression\n",
    "\n",
    "$ if [[ $str =~ [0-9]+\\.[0-9]+ ]]; then\n",
    "    # do something\n",
    "fi\n",
    "\n",
    "This one-liner tests if the string $str matches regex [0-9]+\\.[0-9]+, which means match a number followed by a dot followed by number. The format for regular expressions is described in man 3 regex.\n",
    "\n",
    "12. Find the length of the string\n",
    "\n",
    "$ echo ${#str}\n",
    "\n",
    "Here we use parameter expansion ${#str} which returns the length of the string in variable str. Really simple.\n",
    "\n",
    "13. Extract a substring from a string\n",
    "\n",
    "$ str=\"hello world\"\n",
    "$ echo ${str:6}\n",
    "\n",
    "This one-liner extracts world from hello world. It uses the substring expansion. In general substring expansion looks like ${var:offset:length}, and it extracts length characters from var starting at index offset. In our one-liner we omit the length that makes it extract all characters starting at offset 6.\n",
    "\n",
    "Here is another example:\n",
    "\n",
    "$ echo ${str:7:2}\n",
    "\n",
    "Output:\n",
    "\n",
    "or\n",
    "14. Uppercase a string\n",
    "\n",
    "$ declare -u var\n",
    "$ var=\"foo bar\"\n",
    "\n",
    "The declare command in bash declares variables and/or gives them attributes. In this case we give the variable var attribute -u, which upper-cases its content whenever it gets assigned something. Now if you echo it, the contents will be upper-cased:\n",
    "\n",
    "$ echo $var\n",
    "\n",
    "FOO BAR\n",
    "Note that -u argument was introduced in bash 4. Similarly you can use another feature of bash 4, which is the ${var^^} parameter expansion that upper-cases a string in var:\n",
    "\n",
    "$ str=\"zoo raw\"\n",
    "$ echo ${str^^}\n",
    "\n",
    "Output:\n",
    "\n",
    "ZOO RAW\n",
    "15. Lowercase a string\n",
    "\n",
    "$ declare -l var\n",
    "$ var=\"FOO BAR\"\n",
    "\n",
    "Similar to the previous one-liner, -l argument to declare sets the lower-case attribute on var, which makes it always be lower-case:\n",
    "\n",
    "$ echo $var\n",
    "\n",
    "foo bar\n",
    "The -l argument is also available only in bash 4 and later.\n",
    "\n",
    "Another way to lowercase a string is to use ${var,,} parameter expansion:\n",
    "\n",
    "$ str=\"ZOO RAW\"\n",
    "$ echo ${str,,}\n",
    "\n",
    "Output:\n",
    "\n",
    "zoo raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Part IV: Working with history\n",
    "\n",
    "1. Erase all shell history\n",
    "\n",
    "$ rm ~/.bash_history\n",
    "\n",
    "Bash keeps the shell history in a hidden file called .bash_history. This file is located in your home directory. To get rid of the history, just delete it.\n",
    "\n",
    "Note that if you logout after erasing the shell history, this last rm ~/.bash_history command will be logged. If you want to hide that you erased shell history, see the next one-liner.\n",
    "\n",
    "2. Stop logging history for this session\n",
    "\n",
    "$ unset HISTFILE\n",
    "\n",
    "The HISTFILE special bash variable points to the file where the shell history should be saved. If you unset it, bash won't save the history.\n",
    "\n",
    "Alternatively you can point it to /dev/null,\n",
    "\n",
    "$ HISTFILE=/dev/null\n",
    "\n",
    "3. Don't log the current command to history\n",
    "\n",
    "Just start the command with an extra space:\n",
    "\n",
    "$  command\n",
    "\n",
    "If the command starts with an extra space, it's not logged to history.\n",
    "\n",
    "Note that this only works if the HISTIGNORE variable is properly configured. This variable contains : separated values of command prefixes that shouldn't be logged.\n",
    "\n",
    "For example to ignore spaces set it to this:\n",
    "\n",
    "HISTIGNORE=\"[ ]*\"\n",
    "My HISTIGNORE looks like this:\n",
    "\n",
    "HISTIGNORE=\"&:[ ]*\"\n",
    "The ampersand has a special meaning - don't log repeated commands.\n",
    "\n",
    "4. Change the file where bash logs command history\n",
    "\n",
    "$ HISTFILE=~/docs/shell_history.txt\n",
    "\n",
    "Here we simply change the HISTFILE special bash variable and point it to ~/docs/shell_history.txt. From now on bash will save the command history in that file.\n",
    "\n",
    "5. Add timestamps to history log\n",
    "\n",
    "$ HISTTIMEFORMAT=\"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "If you set the HISTTIMEFORMAT special bash variable to a valid date format (see man 3 strftime) then bash will log the timestamps to the history log. It will also display them when you call the history command (see the next one-liner).\n",
    "\n",
    "6. Show the history\n",
    "\n",
    "$ history\n",
    "\n",
    "The history command displays the history list with line numbers. If HISTTIMEFORMAT is set, it also displays the timestamps.\n",
    "\n",
    "7. Show the last 50 commands from the history\n",
    "\n",
    "$ history 50\n",
    "\n",
    "If you specify a numeric argument, such as 50, to history, it prints the last 50 commands from the history.\n",
    "\n",
    "7. Show the top 10 most used commands from the bash history\n",
    "\n",
    "$ history |\n",
    "    sed 's/^ \\+//;s/  / /' |\n",
    "    cut -d' ' -f2- |\n",
    "    awk '{ count[$0]++ } END { for (i in count) print count[i], i }' |\n",
    "    sort -rn |\n",
    "    head -10\n",
    "\n",
    "This one-liner combines bash with sed, cut, awk, sort and head. The perfect combination. Let's walk through this to understand what happens. Let's say the output of history is:\n",
    "\n",
    "$ history\n",
    "    1  rm .bash_history \n",
    "    2  dmesg\n",
    "    3  su -\n",
    "    4  man cryptsetup\n",
    "    5  dmesg\n",
    "\n",
    "First we use the sed command to remove the leading spaces and convert the double space after the history command number to a single space:\n",
    "\n",
    "$ history | sed 's/^ \\+//;s/  / /'\n",
    "1 rm .bash_history \n",
    "2 dmesg\n",
    "3 su -\n",
    "4 man cryptsetup\n",
    "5 dmesg\n",
    "\n",
    "Next we use cut to remove the first column (the history numbers):\n",
    "\n",
    "$ history |\n",
    "    sed 's/^ \\+//;s/  / /' |\n",
    "    cut -d' ' -f2-\n",
    "\n",
    "rm .bash_history \n",
    "dmesg\n",
    "su -\n",
    "man cryptsetup\n",
    "dmesg\n",
    "\n",
    "Next we use awk to record how many times each command has been seen:\n",
    "\n",
    "$ history |\n",
    "    sed 's/^ \\+//;s/  / /' |\n",
    "    cut -d' ' -f2- |\n",
    "    awk '{ count[$0]++ } END { for (i in count) print count[i], i }'\n",
    "\n",
    "1 rm .bash_history \n",
    "2 dmesg\n",
    "1 su -\n",
    "1 man cryptsetup\n",
    "\n",
    "Then we sort the output numerically and reverse it:\n",
    "\n",
    "$ history |\n",
    "    sed 's/^ \\+//;s/  / /' |\n",
    "    cut -d' ' -f2- |\n",
    "    awk '{ count[$0]++ } END { for (i in count) print count[i], i }' |\n",
    "    sort -rn\n",
    "\n",
    "2 dmesg\n",
    "1 rm .bash_history \n",
    "1 su -\n",
    "1 man cryptsetup\n",
    "\n",
    "Finally we take the first 10 lines that correspond to 10 most frequently used commands:\n",
    "\n",
    "$ history |\n",
    "    sed 's/^ \\+//;s/  / /' |\n",
    "    cut -d' ' -f2- |\n",
    "    awk '{ count[$0]++ } END { for (i in count) print count[i], i }' |\n",
    "    sort -rn |\n",
    "    head -10\n",
    "\n",
    "Here is what my 10 most frequently used commands look like:\n",
    "\n",
    "2172 ls\n",
    "1610 gs\n",
    "252 cd ..\n",
    "215 gp\n",
    "213 ls -las\n",
    "197 cd projects\n",
    "155 gpu\n",
    "151 cd\n",
    "119 gl\n",
    "119 cd tests/\n",
    "\n",
    "Here I've gs that's an alias for git status, gp is git push, gpu is git pull and gl is git log.\n",
    "\n",
    "8. Execute the previous command quickly\n",
    "\n",
    "$ !!\n",
    "\n",
    "That's right. Type two bangs. The first bang starts history substitution, and the second one refers to the last command. Here is an example:\n",
    "\n",
    "$ echo foo\n",
    "foo\n",
    "$ !!\n",
    "foo\n",
    "\n",
    "Here the echo foo command was repeated.\n",
    "\n",
    "It's especially useful if you wanted to execute a command through sudo but forgot. Then all you've to do is run:\n",
    "\n",
    "$ rm /var/log/something\n",
    "rm: cannot remove `/var/log/something': Permission denied\n",
    "$\n",
    "$ sudo !!   # executes `sudo rm /var/log/something`\n",
    "\n",
    "9. Execute the most recent command starting with the given string\n",
    "\n",
    "$ !foo\n",
    "\n",
    "The first bang starts history substitution, and the second one refers to the most recent command starting with foo.\n",
    "\n",
    "For example,\n",
    "\n",
    "$ echo foo\n",
    "foo\n",
    "$ ls /\n",
    "/bin /boot /home /dev /proc /root /tmp\n",
    "$ awk -F: '{print $2}' /etc/passwd\n",
    "...\n",
    "$ !ls\n",
    "/bin /boot /home /dev /proc /root /tmp\n",
    "\n",
    "Here we executed commands echo, ls, awk, and then used !ls to refer to the ls / command.\n",
    "\n",
    "10. Open the previous command you executed in a text editor\n",
    "\n",
    "$ fc\n",
    "\n",
    "Fc opens the previous command in a text editor. It's useful if you've a longer, more complex command and want to edit it.\n",
    "\n",
    "For example, let's say you've written a one-liner that has an error, such as:\n",
    "\n",
    "$ for wav in wav/*; do mp3=$(sed 's/\\.wav/\\.mp3/' <<< \"$wav\"); ffmpeg -i \"$wav\" \"$m3p\"; done\n",
    "\n",
    "And you can't see what's going on because you've to scroll around, then you can simply type fc to load it in your text editor, and then quickly find that you mistyped mp3 at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ytube-dl.sh下载脚本\n",
    "#使用  ./ytube-dl.sh \"https://www.youtube.com/网址\"\n",
    "#!/usr/bin/env bash\n",
    "video_url=\"${1}\"\n",
    "out_file_name=\"${2:-out.video}\"\n",
    "curl \\\n",
    "  -H 'Upgrade-insecure-requests: 1' \\\n",
    "  -H 'Cache-control: max-age=0' \\\n",
    "  -H 'Accept-language: en-US,en;q=0.8,bn;q=0.6' \\\n",
    "  -H 'Accept-encoding: gzip, deflate, sdch' \\\n",
    "  -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' \\\n",
    "  -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36' \\\n",
    "  \"${video_url}\" \\\n",
    "| gunzip \\\n",
    "| egrep -o 'https%3A%2F%2F[^\\.]*\\.googlevideo.com%2F[^,\\]*' \\\n",
    "| perl -pe 's/\\%(\\w\\w)/chr hex $1/ge' \\\n",
    "| head -n1 \\\n",
    "| xargs wget -O \"${out_file_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://www.bashoneliners.com/\n",
    "用来搜索一行命令\n",
    "https://github.com/adrian011494/Bash-One-Liners\n",
    "python liners 命令名\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace the necesssary fields\n",
    "ldapsearch -v -x -H ldap://10.0.0.39 -D CN=106114104,OU=2014,OU=UG,OU=CSE,DC=octa,DC=edu -W -b dc=octa,dc=edu -s sub \"CN=106114104\"\n",
    "\n",
    "cat $path |cut -d ' ' -f 1|grep -v '^$'|sort|uniq -c\n",
    "\n",
    "smbclient //octa.edu/A4-4515X <octa-pass> -U <Roll-No> -W octa.edu -I 10.0.0.38 -c \"print <filename>; exit;\"\n",
    "\n",
    "seq 106114001 106114105|xargs -I {} sshpass -p {} ssh -o StrictHostKeyChecking=no {}@10.0.0.84 \"chmod 755 ~{}\"\n",
    "\n",
    "sudo mount -t cifs //nas/<ug or pg>/<dept>/<roll no> <path_to_local_directory> -o username=<roll no>,workgroup=octa.edu\n",
    "# provide octa password on prompt\n",
    "\n",
    "---------------\n",
    "subl\n",
    "gnome-terminal --tab -e \"bash\" --working-directory=~/Desktop/Coding --tab -e \"bash\"\n",
    "google-chrome --incognito &\n",
    "google-chrome google.com youtube.com facebook.com &\n",
    "gnome-open .\n",
    "vlc /media/venkkatesh/Games\\ and\\ Movies/Songs/Bill/*.mp3 \n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top -b -n1 -u test 'Show Processes and memory for a user'\n",
    "find /proc/sys -type f | for line in $(cat -);do echo $line;cat $line;done  show all kernel parameter values\n",
    "awk /Hug.*Tot/{print } /proc/meminfo  Number of hugepages\n",
    "awk /Hug.*siz/{print } /proc/meminfo  Size of hugepages\n",
    "awk /Hug.*Fre/{print } /proc/meminfo  Available of hugepages\n",
    "ps -ef | awk NR { print } | sort -u   | wc -l  Count the unique users on system\n",
    "ps -ef | awk NR { print } | sort -u   | uniq -c Show how many processes each logon is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List all changed, added or deleted files.\n",
    "\n",
    "git status -s | awk '{ print $2 }'\n",
    "\n",
    "---------------\n",
    "# List git commits made on Monday.\n",
    "\n",
    "git log --pretty=format:\"%h%x09%an%x09%ad%x09%s\" | awk '$3 == \"Mon\"'\n",
    "    \n",
    "------------\n",
    "# Kill whatever process is running on the given port.\n",
    "\n",
    "if [ \"$#\" -ne 1 ]; then\n",
    "  echo 'Usage: kill-port PORT'\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "lsof -i:$1 | awk 'NR > 1 { print $2 }' | xargs kill -9\n",
    "    \n",
    "---------------\n",
    "# Concatenate all js files\n",
    "# and separate by line breaks.\n",
    "\n",
    "find . -name '*.js' | xargs -I {} sh -c 'cat {}; echo \"\\n\"'\n",
    "---------------------\n",
    "# Copy all js files from one directory to another.\n",
    "\n",
    "find test_data -name '*.js' | xargs -I {} cp {} copy/\n",
    "\n",
    "------------------\n",
    "# Delete all files from a directory\n",
    "# and ask for confirmation for each file.\n",
    "\n",
    "find test_data -name '*.js' | xargs -p -I {} rm {}\n",
    "\n",
    "------------------------\n",
    "# Print contents of all js files in the current directory.\n",
    "\n",
    "find . -name *.js | xargs cat\n",
    "\n",
    "-----------------\n",
    "# Convert rspec tests from \"should\" and \"stub\" to \"expect\" and \"allow\".\n",
    "\n",
    "convert() {\n",
    "    cat $1 | \\\n",
    "        sed 's/^\\(\\s\\+\\)\\(.*\\)\\(\\.should\\)\\(.*\\)$/\\1expect(\\2\\).to\\4/' | \\\n",
    "        sed 's/^\\(\\s\\+\\)\\(.*\\)\\(\\.stub\\)\\(.*\\)$/\\1allow(\\2).to receive\\4/' \\\n",
    "        > .tmp\n",
    "    mv .tmp $1\n",
    "}\n",
    "export -f convert\n",
    "\n",
    "find spec -name '*_spec.rb' | xargs -I {} bash -c \"convert {}\"\n",
    "\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "循环打印\n",
    "Loops over all strins and print them\n",
    "\n",
    "for one in ABC DEF: do echo $one: done\n",
    "\n",
    "for file in *.txt; do echo $file; done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Create /etc/resolve.conf in cygwin needed for dig/bind utils\n",
    "ipconfig /all > /tmp/foo && head -$(grep -n \"Ethernet adapter Local Area Connection\" /tmp/foo | awk -F: '{ print $1 }') /tmp/foo | tail -$(($(wc -l /tmp/foo | awk '{ print $1 }') - $(grep -n \"Ethernet adapter Local Area Connection\" /tmp/foo | awk -F: '{ print $1 }'))) &> /tmp/bar && tail -$(($(wc -l /tmp/bar | awk '{ print $1 }') - $(grep -n \"DNS Servers\" /tmp/bar | awk -F: '{ print $1 }') + 1)) /tmp/bar | head -3 | awk '{ print \"nameserver \" $NF }' > /etc/resolv.conf && rm /tmp/foo && rm /tmp/bar\n",
    "\n",
    "\n",
    "# Create a large text file, but unlike using truncate or dd if=/dev/(u)random of=blah the file contains only printible characters.\n",
    "echo \"all ur base r belong to us\" >> blah && size=`ls -l blah | awk '{ print $5 }'` && while [ $size -lt 2621440000 ]; do cp blah blah.tmp && cat blah.tmp >> blah && rm blah.tmp && size=`ls -l blah | awk '{ print $5 }'`; done;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poor mans tree size\n",
    "\n",
    "du -h <dir> | grep '[0-9\\.]\\+G'\n",
    "\n",
    "copy and mv\n",
    "\n",
    "cp <fileordir>{,_copy}\n",
    "mv <fileordir>{,_old}\n",
    "\n",
    "random pass gen\n",
    "\n",
    "strings /dev/urandom | grep -o '[[:alnum:]]' | head -n 30 | tr -d '\\n'; echo\n",
    "\n",
    "arp\n",
    "\n",
    "arp-scan -l -I eth0 | sort -k3 > output\n",
    "\n",
    "find largest files\n",
    "\n",
    "find / -type f -print0| xargs -0 ls -s | sort -rn | awk '{size=$1/1024; printf(\"%dMb %s\\n\", size,$2);}' | head -5\n",
    "\n",
    "update resolution\n",
    "\n",
    "sudo grubby --update-kernel=ALL --args=\"video=hyperv_fb:2560x1440\"\n",
    "© 2017 GitHub, Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -al . | awk '{print $9}' | echo $(basename $(cat -)) | tr -d '\\n' | pbcopy\n",
    "\n",
    "\n",
    "tail -f /var/log/my-log.log \\\n",
    "| awk '{ print strftime (\"%Y-%m-%d %H:%M:%S \"), $0, fflush();}'\n",
    "\n",
    "# Prints out source and port of rejected (iptables) connection packets\n",
    "cat /var/log/syslog | grep SYN | sed -n 's/.*DST=\\([0-9\\.]*\\).*DPT=\\([0-9]*\\).*/\\1 \\2/p' \n",
    "\n",
    "# Prints unique lists of CIDR for rejected firewall ports (good for whitelisting)\n",
    "cat /var/log/syslog | grep SYN | sed -n 's/.*DST=\\([0-9\\.]*\\).*DPT=\\([0-9]*\\).*/\\1 \\2/p' | sort | uniq | ( while read ip port; do cidr=`whois $ip | grep CIDR`; name=`whois $ip|grep NetName`; echo $cidr $port $name; done ) | sort | uniq\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/gaursagar/bash-one-liners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  ############# removing soft line wrap from fasta #############\n",
    "$ awk '!/^>/ { printf \"%s\", $0; n=\"\" } /^>/ { print n $0; n = \"\" } END { printf \"%s\", n }' in.fa > out.fa \n",
    "\n",
    "  ############# splitting fastq to fasta with sed #############\n",
    "$ sed -n '1~4s/^@/>/p;2~4p' in.fq > out.fa\n",
    "\n",
    "\n",
    "  ############# replacing ^M carriage returns that excel likes to put in with newline characters #############\n",
    "$ tr \"\\r\" \"\\n\" < paired_dists.csv > paired_dists_fixed.csv\n",
    "\n",
    "\n",
    "  ############# quick de-interleave interleaved fastq files #############\n",
    "$ paste - - - - - - - - < interleaved.fastq | tee >(cut -f 1-4 | tr '\\t' '' > reads1.fastq) | cut -f 5-8 | tr '\\t' '' > reads2.fastq\n",
    "\n",
    "\n",
    "  ############# replacing newline characters with mac bash's finicky sed #############\n",
    "$ sed -e ':a' -e 'N' -e '$!ba' -e 's///g' in.txt > out.txt\n",
    "\n",
    "\n",
    "  ############# example to download gene nucleotide sequences from ncbi by gene id #############\n",
    "$ perl -e 'use LWP::Simple;getstore(\"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nucleotide&rettype=fasta&retmode=text&id=\".join(\",\",qw(6701965 6701969 6702094 6702105 6702160)),\"seqs.fasta\");'\n",
    "\n",
    "\n",
    "  ############# count number of bases in a fasta file #############\n",
    "$ grep -v \">\" file.fa | wc | awk '{print $3-$1}'\n",
    "\n",
    "\n",
    "  ############# replace full word only with sed on mac (enclose string with \"[[:>:]]...[[:<:]]\") #############\n",
    "$ sed \"s/[[:<:]]1[[:>:]]/A/\" test.txt\n",
    "  # this example replaces 1 with A only when 1 is full word, analogous to -w flag in most bash shells outside of mac\n",
    "\n",
    "\n",
    "  ############# add '>' to every other line #############\n",
    "$ sed 's/^/>/;n' test.fa\n",
    "\n",
    "  ############# interleave two files line-by-line #############\n",
    "$ paste -d'\\n' temp_headers temp_seqs > new_fasta.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Emptying a file\n",
    "\n",
    "> file.txt\n",
    "It is shorter than the commonly used:\n",
    "\n",
    "echo '' > file.txt\n",
    "Filesize in bytes\n",
    "\n",
    "Instead of trying to parse ls -l output, it's easier to use stat:\n",
    "\n",
    "stat -c %s file.txt\n",
    "\n",
    "Prepend all lines with a string\n",
    "\n",
    "Prepend hello to all lines:\n",
    "\n",
    "sed 's/^/hello /'\n",
    "Example:\n",
    "\n",
    "$ echo -e 'one\\ntwo\\nthree' | sed 's/^/hello /'\n",
    "hello one\n",
    "hello two\n",
    "hello three\n",
    "Output only N-th line\n",
    "\n",
    "Output only tenth line:\n",
    "\n",
    "sed -n 10p\n",
    "Output only lines M through N\n",
    "\n",
    "Output only lines 10, 11, …, 20:\n",
    "\n",
    "sed -n 10,20p\n",
    "Remove N-th line from output\n",
    "\n",
    "Output all lines but the tenth:\n",
    "\n",
    "sed 10d\n",
    "Output even/odd/N-th lines only\n",
    "\n",
    "Even lines only:\n",
    "\n",
    "sed -n '2~2p'\n",
    "Odd lines only:\n",
    "\n",
    "sed -n '1~2p'\n",
    "Generic form:\n",
    "\n",
    "sed -n 'A~Bp'\n",
    "This starts with the A-th line and then outputs every B-th line.\n",
    "\n",
    "Output all lines between two matches\n",
    "\n",
    "Output all lines from the line containing A through the line containing B. The two boundary lines are included:\n",
    "\n",
    "awk '/A/,/B/'\n",
    "Example:\n",
    "\n",
    "$ echo -e 'one\\ntwo\\nthree\\nfour\\nfive\\nsix' | awk /two/,/five/\n",
    "two\n",
    "three\n",
    "four\n",
    "five\n",
    "Output unique lines without sorting\n",
    "\n",
    "awk '!x[$0]++'\n",
    "(not suitable for large files)\n",
    "\n",
    "Example:\n",
    "\n",
    "$ echo -e 'three\\ntwo\\ntwo\\none\\nthree\\nthree\\none' | awk '!x[$0]++'\n",
    "three\n",
    "two\n",
    "one\n",
    "Repeat a string N times\n",
    "\n",
    "echo -e $_{1..N}'\\bSTRING'\n",
    "(where N is the count and STRING is the word to repeat)\n",
    "\n",
    "Without \\b the words will be separated by spaces:\n",
    "\n",
    "echo $_{1..N}'STRING'\n",
    "Example: Repeat the word Hello 6 times:\n",
    "\n",
    "$ echo -e $_{1..6}'\\bHello'\n",
    "HelloHelloHelloHelloHelloHello\n",
    "Bash variable manipulation\n",
    "\n",
    "Search and replace\n",
    "\n",
    "Replace the first dot with an underscore:\n",
    "\n",
    "$ var=a.b.c.d; echo ${var/./_}\n",
    "a_b.c.d\n",
    "Replace all dots with underscores:\n",
    "\n",
    "$ var=a.b.c.d; echo ${var//./_}\n",
    "a_b_c_d\n",
    "Search and remove\n",
    "\n",
    "Remove the first dot:\n",
    "\n",
    "$ var=a.b.c.d; echo ${var/.}\n",
    "ab.c.d\n",
    "Remove all dots:\n",
    "\n",
    "$ var=a.b.c.d; echo ${var//.}\n",
    "abcd\n",
    "Conditional replace\n",
    "\n",
    "Replace the first character with an X if it is an a/b:\n",
    "\n",
    "$ var=a.b.c.d; echo ${var/#a/X}\n",
    "X.b.c.d\n",
    "\n",
    "$ var=a.b.c.d; echo ${var/#b/X}\n",
    "a.b.c.d\n",
    "Replace the last character with an X if it is an d/e:\n",
    "\n",
    "$ var=a.b.c.d; echo ${var/%d/X}\n",
    "a.b.c.X\n",
    "\n",
    "$ var=a.b.c.d; echo ${var/%e/X}\n",
    "a.b.c.d\n",
    "Strip the beginning/end according to a pattern\n",
    "\n",
    "Remove everything after the last dot (ungreedy pattern):\n",
    "\n",
    "$ var=a.b.c.d; echo ${var%.*}\n",
    "a.b.c\n",
    "Remove everything after the first dot (greedy pattern):\n",
    "\n",
    "$ var=a.b.c.d; echo ${var%%.*}\n",
    "a\n",
    "Remove everything before the first dot (ungreedy pattern):\n",
    "\n",
    "$ var=a.b.c.d; echo ${var#*.}\n",
    "b.c.d\n",
    "Remove everything before the last dot (greedy pattern):\n",
    "\n",
    "$ var=a.b.c.d; echo ${var##*.}\n",
    "d\n",
    "Convert to lowercase/uppercase\n",
    "\n",
    "Convert the first character to uppercase/lowercase:\n",
    "\n",
    "$ var=abcd; echo ${var^}\n",
    "Abcd\n",
    "\n",
    "$ var=ABCD; echo ${var,}\n",
    "aBCD\n",
    "Convert all characters to uppercase/lowercase:\n",
    "\n",
    "$ var=abcd; echo ${var^^}\n",
    "ABCD\n",
    "\n",
    "$ var=ABCD; echo ${var,,}\n",
    "abcd\n",
    "Substring\n",
    "\n",
    "All but the first 2 characters:\n",
    "\n",
    "$ var=abcd; echo ${var:2}\n",
    "cd\n",
    "3 characters, starting from the 2nd:\n",
    "\n",
    "$ var=abcdef; echo ${var:2:3}\n",
    "cde\n",
    "The last 2 characters:\n",
    "\n",
    "$ var=abcdef; echo ${var: -2}\n",
    "ef\n",
    "(The space before the minus/dash is necessary since :- means: use a default value if the variable is not set.)\n",
    "\n",
    "String length\n",
    "\n",
    "$ var=abcdef; echo ${#var}\n",
    "6\n",
    "Other utilities\n",
    "\n",
    "Pick a random line\n",
    "\n",
    "shuf -n 1\n",
    "shuf shuffles all lines, -n 1 outputs only the first line.\n",
    "\n",
    "Output lines in reverse order\n",
    "\n",
    "Type cat in reverse:\n",
    "\n",
    "tac\n",
    "Example:\n",
    "\n",
    "$ echo -e 'one\\ntwo\\nthree' | tac\n",
    "three\n",
    "two\n",
    "one\n",
    "Diff two unsorted files without creating temporary files\n",
    "\n",
    "diff <(sort file1.txt) <(sort file2.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort by column\n",
    "\tsort -n -k 1\n",
    "\n",
    "# Cut out columns 1, 2, 3, 4, and 5\n",
    "\tcut -f1,2,3,4,5 filename \n",
    " \n",
    "# Sort, Uniq, Count by column 3\n",
    "\tmore filename | sort -s -n -k 3 | uniq -f 3 -c \n",
    "\n",
    "# Differences between two columns in 2 files\n",
    "\tawk 'NR==FNR{c[$4]++;next};c[$4] == 0'\t\n",
    "\tawk -F, 'FILENAME != ARGV[ARGC-1] {keys[$6]; next} !($6 in keys)'\n",
    "\tawk 'NR == FNR { a[$1,$2] = $0; next }  { delete a[$1, $2] } END { for (i in a) print a[i] }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Count lines in multiple files and display lines per file\n",
    "  grep -c filenames\n",
    "\n",
    "#Mutation spectrum (base pair change) of SNVs in a BED file\n",
    "\tcut -f4,5 filename | sort | uniq -c\t\n",
    "\n",
    "#Find difference between 2 files\n",
    "\tawk 'FNR==NR{f[$1];next}(!($1 in f)) ' file1 file2\n",
    "\n",
    "#Find pattern and display n lines above pattern (n=10)\n",
    "\tgrep \"pattern\" filename -B 10 \n",
    "\n",
    "#Find pattern and display n lines below pattern (n=10)\n",
    "\tgrep \"pattern\" filename -A 10 \n",
    "\n",
    "#Combine files\n",
    "\tcat file1 file2 file3 > newfile\n",
    "\n",
    "#Replace ^M with line break in VI text editor\n",
    "\t%s/^M/\\r/g\n",
    "\n",
    "#Replace space in the beginning of line VI text editor\n",
    "\t%s/^ \\+//\n",
    "\n",
    "#Compare file1 and file2; suppress lines unique to FILE1 \n",
    "\tcomm -1 file1 file2 > file3\n",
    "\n",
    "#Compare file1 and file2, suppress lines unique to FILE2\n",
    "\tcomm -2 file1 file2 > file3\n",
    "\n",
    "#Compare file1 and file2, suppress lines that appear in both files\n",
    "\tcomm -3 file1 file2 > file3\n",
    "\n",
    "#Extract last field per line\n",
    "\tawk '{ print $NF }' filename\n",
    "\t\n",
    "#Mutation spectrum (base Pair change) of SNVs in a BED file\n",
    "\tcut -f4,5 filename | sort | uniq -c\n",
    "\n",
    "#Extract the Nth string in a line\n",
    "\tcut -c Nth\t\n",
    "\t\n",
    "#Unzip multiple tar files\n",
    "\tfor file in *.tar.gz; do tar -zxf $file; done\n",
    "\n",
    "#Extract the 8th line of single file\n",
    "\tsed '8q;d' filename\t\n",
    "\n",
    "#Extract the 8th line of multiple files\t\n",
    "\tawk 'FNR == 8' *filenames\n",
    "\t\n",
    "#Find 2 patterns in the same file\n",
    "\tcat filename | grep -e \"pattern1\"  -e \"pattern2\"\n",
    "\t\n",
    "#Append output to beginning of file\n",
    "\tcat <(command) filename >tmp && mv tmp filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count number of sequences in a fasta file\n",
    "\tgrep -c \"^>\" file.fa\n",
    "\n",
    "#Sequence length distribution in a Fastq File\n",
    "\tawk 'NR%4 == 2 {lengths[length($0)]++} END {for (l in lengths) {print l, lengths[l]}}' file.fastq\n",
    "\tcat file.fastq | awk '{if(NR%4==2) print length($1)}' | sort -n | uniq -c \n",
    "\n",
    "#number of reads in a bam file\n",
    "\tsamtools idxstats in.bam | awk '{s+=$3+$4} END {print s}'\n",
    "\n",
    "#number of mapped reads in a bam file\n",
    "\tsamtools idxstats in.bam | awk '{s+=$3} END {print s}'\n",
    "\n",
    "#convert fasta file to BLAST database\n",
    "\tformatdb -p F -i fasta-file -n name-of-BLASTdb\n",
    "\n",
    "#convert SAM file to BAM file \n",
    "\tsamtools view in.bam > out.sam \n",
    "\n",
    "#convert BAM file to SAM file \n",
    "\tsamtools view in.sam > out.sam \n",
    "\t\n",
    "#Extract fasta sequences with IDs in a seperate file\n",
    "\tperl -ne 'if(/^>(\\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' ids.file fasta.file\n",
    "\t\n",
    "#Find \".\" replace with nothing\n",
    "\tsed  \"s/\\.//g\" file.in >> file.out \n",
    "\n",
    "#Find space replace with pipe \"|\"\n",
    "\tsed 's/[ \\t]/|/g' file.in >> file.out \n",
    "\n",
    "#Delete lines containing obsolete\n",
    "\tsed '/obsolete/d' file.in >> file.out \n",
    "\n",
    "#Clean fasta file downloaded from NCBI to retain only species names \n",
    "#Open file in favortie text editor\n",
    "#Search with the following string and replace with nothing\n",
    "\tgi\\|[0-9]+\\|[A-Z]+\\|.+\\|.+\\[ \n",
    "\t\n",
    "#Remove empty headers with no sequences in fasta file\n",
    "\tawk 'BEGIN {RS = \">\" ; FS = \"\\n\" ; ORS = \"\"} $2 {print \">\"$0}' input.fasta > output.fasta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Total number of variants in vcf file\n",
    "\tgrep -v \"#\" filename | wc -l\n",
    "\t\t \n",
    "#Total number of variants in vcf file that passed filtering\n",
    "\tawk '{if($7==\"PASS\") print}' filename | wc -l\n",
    "\n",
    "#Number of unique SNVs across all sample in a Monovar output file\n",
    "\tcut -f20 MonoVar_output_file.vcf | sed 1,20d | sed 's/[^1,2]//g'  | awk '{ print length }' | sort | wc -l\n",
    "\t\n",
    "#Number of unique SNVs in at least 2 samples within a Monovar output file\n",
    "\tcut -f20 MonoVar_output_file.vcf   | sed 1,20d | sed 's/[^1,2]//g'  | awk '{ print length }' | sort | awk '$1> 1' | wc -l\n",
    "\t\n",
    "#Number of unique SNVs in at least 3 samples within a Monovar output file\n",
    "\tcut -f20 MonoVar_output_file.vcf   | sed 1,20d | sed 's/[^1,2,3]//g'  | awk '{ print length }' | sort | awk '$1> 2' | wc -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "%%ruby\n",
    "puts 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "在一个变量中把多个空白合并成一个空白，用 Perl 是这么一句话：\n",
    "$text =~ s/(\\s)+/( )/g;\n",
    "\n",
    "ls -l | awk '$1==\"-rw-r--r--\" {x += $5} END {print \"Total bytes = \" x}'\n",
    "# sum the file sizes in bytes of all files in a directory\n",
    "\n",
    "# Shell one liners, using awk or something else!\n",
    "\n",
    "# Give a 2 column, tab separated list of read no. and read length\n",
    "zcat whatever.fastq.gz | paste - - - - | awk '{print NR \" \" (length($3))}'\n",
    "\n",
    "\n",
    "# Count total reads in fastq file\n",
    "zcat whatever.fastq.gz | wc -l | awk '{print $1/4}' \n",
    "\n",
    "# Change extension of multiple files at once.\n",
    "# In below example, the extension changes from *.scafSeq to *.fa\n",
    "for f in *.scafSeq; do mv \"$f\" \"$(basename \"$f\" .scafSeq).fa\"; done\n",
    "\n",
    "\n",
    "# get A T G C counts for all sequences from a multi fasta file\n",
    "echo -e \"seq_id\\tA\\tU\\tG\\tC\"; while read line; do echo $line | grep \">\" | sed 's/>//g'; for i in A U G C;do echo $line | grep -v \">\" | grep -o $i | wc -l | grep -v \"^0\"; done; done < test.fa | paste - - - - -\n",
    "\n",
    "#counting number of sequences in a fasta file:\n",
    "grep -c \"^>\" file.fa\n",
    "\n",
    "#add something to end of all header lines:\n",
    "sed 's/>.*/&WHATEVERYOUWANT/' file.fa > outfile.fa\n",
    "\n",
    "#clean up a fasta file so only first column of the header is outputted:\n",
    "awk '{print $1}' file.fa > output.fa\n",
    "\n",
    "Print only line number 101 in file:\n",
    "awk 'NR==101' file\n",
    "\n",
    "Print lines 101-202 in file:\n",
    "awk 'NR==101, NR==202' file\n",
    "\n",
    "Print every 4th line in a file:\n",
    "awk '0 == NR % 4' file\n",
    "\n",
    "Print header with column/field numbers in a tab-separated file:\n",
    "awk -F \"\\t\" '{ for (f=1; f<=NF; f++) print f\":\"$f; exit }' file\n",
    "\n",
    "Rename multiple files (/bin/sh executes):\n",
    "ls * | awk '{ print \"mv \"$1\" \"$1 }' | sed 's/to_replace/replace_with/2' | /bin/sh\n",
    "\n",
    "Remove duplicate, nonconsecutive lines (no need to sort first):\n",
    "awk '!a[$0]++' file\n",
    "\n",
    "Put every other line next to the one above, separated by tab:\n",
    "awk 'ORS=NR%2 ? \"\\t\" : \"\\n\"' file\n",
    "\n",
    "Print section of file between two regexes:\n",
    "awk '/regex1/, /regex2/' file\n",
    "\n",
    "Print average of third column:\n",
    "awk '{ sum += $3 } END { if (NR > 0) print sum / NR }' file\n",
    "\n",
    "Sort on 3rd column, then print fields 2 through 5 of a tabular file, and align output:\n",
    "sort -nk 3 file | cut -f 2-5 | column -t\n",
    "\n",
    "Print number of files present in each subdirectory of the current directory:\n",
    "find . -maxdepth 1 -type d -exec bash -c \"echo -ne '{} '; ls '{}' | wc -l\" \\;\n",
    "\n",
    "Check if the same file exists in all directories:\n",
    "for d in ./bin*; do [[ -f ./$d/RAxML_bootstrap-out ]] && echo \"File exist in dir $d\" || echo \"File does not exist in dir $d\"; done\n",
    "\n",
    "Find directories that are missing files with the word masked in their name:\n",
    "find base_dir -type d '!' -exec sh -c 'ls -1 \"{}\" | grep -q \"*masked*\"' ';' -print\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sed\n",
    "\n",
    " # spasi dobel untuk sebuah berkas\n",
    " sed G\n",
    "\n",
    " # spasi dobel sebuah berkas yang sudah memiliki baris-baris kosong di\n",
    " # dalamnya. Berkas keluaran nanti berisi tidak lebih dari satu baris \n",
    " # kosong di antara baris-baris teks\n",
    " sed '/^$/d;G'\n",
    "\n",
    "\n",
    " # spasi tripel untuk sebuah berkas\n",
    " sed 'G;G'\n",
    "\n",
    " # undo spasi-dobel (anggap baris-baris bernomor genap selalu kosong)\n",
    " sed 'n;d'\n",
    " \n",
    " # sisipkan satu baris kosong di atas setiap baris yang cocok dengan \"regex\"\n",
    " sed '/regex/{x;p;x;}'\n",
    "\n",
    " # sisipkan sebuah baris kosong di bawah setiap baris yang cocok dengan \"regex\"\n",
    " sed '/regex/G'\n",
    "\n",
    " # insert a blank line above and below every line which matches \"regex\"\n",
    " sed '/regex/{x;p;x;G;}'\n",
    "\n",
    "PENOMORAN:\n",
    "\n",
    " # number each line of a file (simple left alignment). Using a tab (see\n",
    " # note on '\\t' at end of file) instead of space will preserve margins.\n",
    " sed = filename | sed 'N;s/\\n/\\t/'\n",
    "\n",
    " # number each line of a file (number on left, right-aligned)\n",
    " sed = filename | sed 'N; s/^/     /; s/ *\\(.\\{6,\\}\\)\\n/\\1  /'\n",
    "\n",
    " # number each line of file, but only print numbers if line is not blank\n",
    " sed '/./=' filename | sed '/./N; s/\\n/ /'\n",
    "\n",
    " # hitung baris (meniru \"wc -l)\n",
    " sed -n '$='\n",
    "\n",
    "KONVERSI TEKS DAN SUBSTITUSI:\n",
    "\n",
    " # DI LINGKUNGAN UNIX: convert DOS newlines (CR/LF) to Unix format.\n",
    " sed 's/.$//'               # assumes that all lines end with CR/LF\n",
    " sed 's/^M$//'              # in bash/tcsh, press Ctrl-V then Ctrl-M\n",
    " sed 's/\\x0D$//'            # works on ssed, gsed 3.02.80 or higher\n",
    "\n",
    " # DI LINGKUNGAN UNIX: convert Unix newlines (LF) to DOS format.\n",
    " sed \"s/$/`echo -e \\\\\\r`/\"            # command line under ksh\n",
    " sed 's/$'\"/`echo \\\\\\r`/\"             # command line under bash\n",
    " sed \"s/$/`echo \\\\\\r`/\"               # command line under zsh\n",
    " sed 's/$/\\r/'                        # gsed 3.02.80 or higher\n",
    "\n",
    " # DI LINGKUNGAN DOS: convert Unix newlines (LF) to DOS format.\n",
    " sed \"s/$//\"                          # method 1\n",
    " sed -n p                             # method 2\n",
    "\n",
    " # DI LINGKUNGAN DOS: convert DOS newlines (CR/LF) to Unix format.\n",
    " # Can only be done with UnxUtils sed, version 4.0.7 or higher. The\n",
    " # UnxUtils version can be identified by the custom \"--text\" switch\n",
    " # which appears when you use the \"--help\" switch. Otherwise, changing\n",
    " # DOS newlines to Unix newlines cannot be done with sed in a DOS\n",
    " # environment. Use \"tr\" instead.\n",
    " sed \"s/\\r//\" infile >outfile         # UnxUtils sed v4.0.7 or higher\n",
    " tr -d \\r <infile >outfile            # GNU tr version 1.22 or higher\n",
    "\n",
    " # hapus spasi atau tab awal di depan setiap baris\n",
    " # dengan kata lain, ratakan kiri seluruh teks\n",
    " sed 's/^[ \\t]*//'                    # see note on '\\t' at end of file\n",
    "\n",
    " # delete trailing whitespace (spaces, tabs) from end of each line\n",
    " sed 's/[ \\t]*$//'                    # see note on '\\t' at end of file\n",
    "\n",
    " # delete BOTH leading and trailing whitespace from each line\n",
    " sed 's/^[ \\t]*//;s/[ \\t]*$//'\n",
    "\n",
    " # sisipkan 5 spasi kosong pada awal setiap baris (membuat page offset)\n",
    " sed 's/^/     /'\n",
    "\n",
    " # ratakan semua teks ke kanan dengan lebar 79 kolom\n",
    " sed -e :a -e 's/^.\\{1,78\\}$/ &/;ta'  # set at 78 plus 1 space\n",
    "\n",
    " # center all text in the middle of 79-column width. In method 1,\n",
    " # spaces at the beginning of the line are significant, and trailing\n",
    " # spaces are appended at the end of the line. In method 2, spaces at\n",
    " # the beginning of the line are discarded in centering the line, and\n",
    " # no trailing spaces appear at the end of lines.\n",
    " sed  -e :a -e 's/^.\\{1,77\\}$/ & /;ta'                     # method 1\n",
    " sed  -e :a -e 's/^.\\{1,77\\}$/ &/;ta' -e 's/\\( *\\)\\1/\\1/'  # method 2\n",
    "\n",
    " # substitusi (find and replace) \"foo\" dengan \"bar\" pada setiap baris\n",
    " sed 's/foo/bar/'             # replaces only 1st instance in a line\n",
    " sed 's/foo/bar/4'            # replaces only 4th instance in a line\n",
    " sed 's/foo/bar/g'            # replaces ALL instances in a line\n",
    " sed 's/\\(.*\\)foo\\(.*foo\\)/\\1bar\\2/' # replace the next-to-last case\n",
    " sed 's/\\(.*\\)foo/\\1bar/'            # replace only the last case\n",
    "\n",
    " # substitusi \"foo\" dengan \"bar\" HANYA untuk baris yang berisi \"baz\"\n",
    " sed '/baz/s/foo/bar/g'\n",
    "\n",
    " # substitusi \"foo\" dengan \"bar\" KECUALI untuk baris yang berisi \"baz\"\n",
    " sed '/baz/!s/foo/bar/g'\n",
    "\n",
    " # ubah \"scarlet\" atau \"ruby\" atau \"puce\" menjadi \"red\"\n",
    " sed 's/scarlet/red/g;s/ruby/red/g;s/puce/red/g'   # most seds\n",
    " gsed 's/scarlet\\|ruby\\|puce/red/g'                # GNU sed only\n",
    "\n",
    " # reverse order of lines (emulates \"tac\")\n",
    " # bug/feature in HHsed v1.5 causes blank lines to be deleted\n",
    " sed '1!G;h;$!d'               # method 1\n",
    " sed -n '1!G;h;$p'             # method 2\n",
    "\n",
    " # balik setiap karakter pada baris sekarang (meniru \"rev\")\n",
    " sed '/\\n/!G;s/\\(.\\)\\(.*\\n\\)/&\\2\\1/;//D;s/.//'\n",
    "\n",
    " # gabung baris-baris berpasangan dua-dua (seperti \"paste\")\n",
    " sed '$!N;s/\\n/ /'\n",
    "\n",
    " # jika sebuah baris berakhiran backslash, bubuhkan baris selanjutnya\n",
    " sed -e :a -e '/\\\\$/N; s/\\\\\\n//; ta'\n",
    "\n",
    " # jika sebuah baris diawali tanda sama dengan, bubuhkan kepada baris\n",
    " # sebelumnya dan ganti \"=\" dengan satu spasi\n",
    " sed -e :a -e '$!N;s/\\n=/ /;ta' -e 'P;D'\n",
    "\n",
    " # tambahkan koma ke string numerik, mengubah \"1234567\" jadi \"1,234,567\"\n",
    " gsed ':a;s/\\B[0-9]\\{3\\}\\>/,&/;ta'                     # GNU sed\n",
    " sed -e :a -e 's/\\(.*[0-9]\\)\\([0-9]\\{3\\}\\)/\\1,\\2/;ta'  # other seds\n",
    "\n",
    " # tambahkan koma ke nomor dengan titik desimal dan tanda minus (GNU sed)\n",
    " gsed -r ':a;s/(^|[^0-9.])([0-9]+)([0-9]{3})/\\1\\2,\\3/g;ta'\n",
    "\n",
    " # tambah baris kosong setiap lima baris (setelah baris 5, 10, 15, dst)\n",
    " gsed '0~5G'                  # GNU sed saja\n",
    " sed 'n;n;n;n;G;'             # sed lainnya\n",
    " \n",
    "PENCETAKAN SELEKTIF DARI BARIS TERTENTU:\n",
    "\n",
    " # cetak 10 baris pertama berkas (meniru perilaku \"head\")\n",
    " sed 10q\n",
    "\n",
    " # cetak baris pertama berkas (meniru \"head -1\")\n",
    " sed q\n",
    "\n",
    " # cetak 10 baris terakhir berkas (meniru \"tail\")\n",
    " sed -e :a -e '$q;N;11,$D;ba'\n",
    "\n",
    " # cetak 2 baris terkahir sebuah berkas (meniru \"tail -2\")\n",
    " sed '$!N;$!D'\n",
    "\n",
    " # cetak baris terakhir sebuah berkas (meniru \"tail -1\")\n",
    " sed '$!d'                    # method 1\n",
    " sed -n '$p'                  # method 2\n",
    "\n",
    " __# cetak baris tepat di bawah sampai ke akhir berkas__\n",
    " sed -e '$!{h;d;}' -e x              # for 1-line files, print blank line\n",
    " sed -e '1{$q;}' -e '$!{h;d;}' -e x  # for 1-line files, print the line\n",
    " sed -e '1{$d;}' -e '$!{h;d;}' -e x  # for 1-line files, print nothing\n",
    "\n",
    " # cetak hanya baris yang sesuai regex (meniru \"grep\")\n",
    " sed -n '/regexp/p'           # method 1\n",
    " sed '/regexp/!d'             # method 2\n",
    "\n",
    " # cetak hanya baris yang TIDAK sesuai regex (meniru \"grep -v\")\n",
    " sed -n '/regexp/!p'          # method 1, corresponds to above\n",
    " sed '/regexp/d'              # method 2, simpler syntax\n",
    "\n",
    " # cetak baris tepat sebelum regex, tetapi bukan yang berisi regex\n",
    " sed -n '/regexp/{g;1!p;};h'\n",
    "\n",
    " # cetak baris tepat setelah regex, tetapi bukan yang berisi regex\n",
    " sed -n '/regexp/{n;p;}'\n",
    "\n",
    " # cetak 1 baris dari konteks sebelum dan sesudah regex, dengan nomor baris\n",
    " # menunjukkan di mana regex terjadi (seperti \"gre[p -A1 -B1\")\n",
    " sed -n -e '/regexp/{=;x;1!p;g;$!N;p;D;}' -e h\n",
    "\n",
    " # grep untuk AAA dan BBB dan CCC (urutan terserah)\n",
    " sed '/AAA/!d; /BBB/!d; /CCC/!d'\n",
    "\n",
    " # grep untuk AAA dan BBB dan CCC (sesuai urutan)\n",
    " sed '/AAA.*BBB.*CCC/!d'\n",
    "\n",
    " # grep untuk AAA atau BBB atau CCC (meniru \"egrep\")\n",
    " sed -e '/AAA/b' -e '/BBB/b' -e '/CCC/b' -e d    # most seds\n",
    " gsed '/AAA\\|BBB\\|CCC/!d'                        # GNU sed only\n",
    "\n",
    " # print paragraph if it contains AAA (blank lines separate paragraphs)\n",
    " # HHsed v1.5 must insert a 'G;' after 'x;' in the next 3 scripts below\n",
    " sed -e '/./{H;$!d;}' -e 'x;/AAA/!d;'\n",
    "\n",
    " # print paragraph if it contains AAA and BBB and CCC (in any order)\n",
    " sed -e '/./{H;$!d;}' -e 'x;/AAA/!d;/BBB/!d;/CCC/!d'\n",
    "\n",
    " # print paragraph if it contains AAA or BBB or CCC\n",
    " sed -e '/./{H;$!d;}' -e 'x;/AAA/b' -e '/BBB/b' -e '/CCC/b' -e d\n",
    " gsed '/./{H;$!d;};x;/AAA\\|BBB\\|CCC/b;d'         # GNU sed only\n",
    "\n",
    " # print only lines of 65 characters or longer\n",
    " sed -n '/^.\\{65\\}/p'\n",
    "\n",
    " # cetak hanya baris yang berisi kurang dari 65 karakter\n",
    " sed -n '/^.\\{65\\}/!p'        # method 1, corresponds to above\n",
    " sed '/^.\\{65\\}/d'            # method 2, simpler syntax\n",
    "\n",
    " # cetak bagian berkas dari regex hingga akhir berkas\n",
    " sed -n '/regexp/,$p'\n",
    "\n",
    " # cetak bagian berkas berdasarkan nomor baris (baris 8-12, inklusif)\n",
    " sed -n '8,12p'               # method 1\n",
    " sed '8,12!d'                 # method 2\n",
    "\n",
    " # cetak baris nomor 52\n",
    " sed -n '52p'                 # method 1\n",
    " sed '52!d'                   # method 2\n",
    " sed '52q;d'                  # method 3, efficient on large files\n",
    "\n",
    " # mulai dari baris tiga, cetak setiap baris ke-7\n",
    " gsed -n '3~7p'               # GNU sed only\n",
    " sed -n '3,${p;n;n;n;n;n;n;}' # other seds\n",
    "\n",
    " # print section of file between two regular expressions (inclusive)\n",
    " # cetak bagian berkas antara dua regex (inklusif)\n",
    " sed -n '/Iowa/,/Montana/p'             # case sensitive\n",
    "\n",
    "PENGHAPUSAN SELEKTIF BARIS TERTENTU:\n",
    "\n",
    " # cetak semua berkas KECUALI bagian di antara 2 regex\n",
    " sed '/Iowa/,/Montana/d'\n",
    "\n",
    " # hapus duplikat, baris berderet dari sebuah berkas (meniru \"uniq\")\n",
    " # Baris pertama dalam sebuah set duplikat dibiarkan, sisanya dihapus\n",
    " sed '$!N; /^\\(.*\\)\\n\\1$/!P; D'\n",
    "\n",
    " # delete duplicate, nonconsecutive lines from a file. Beware not to\n",
    " # overflow the buffer size of the hold space, or else use GNU sed.\n",
    " sed -n 'G; s/\\n/&&/; /^\\([ -~]*\\n\\).*\\n\\1/d; s/\\n//; h; P'\n",
    "\n",
    " # hapus semua baris kecuali baris-baris duplikat (meniru \"uniq\")\n",
    " sed '$!N; s/^\\(.*\\)\\n\\1$/\\1/; t; D'\n",
    "\n",
    " # hapus 10 baris pertama sebuah berkas\n",
    " sed '1,10d'\n",
    "\n",
    " # hapus baris terakhir sebuah berkas\n",
    " sed '$d'\n",
    "\n",
    " # hapus 2 baris terakhir sebuah berkas\n",
    " sed 'N;$!P;$!D;$d'\n",
    "\n",
    " # hapus 10 baris terakhir dari sebuah berkas\n",
    " sed -e :a -e '$d;N;2,10ba' -e 'P;D'   # method 1\n",
    " sed -n -e :a -e '1,10!{P;N;D;};N;ba'  # method 2\n",
    "\n",
    " # hapus setiap baris ke-8\n",
    " gsed '0~8d'                           # GNU sed only\n",
    " sed 'n;n;n;n;n;n;n;d;'                # other seds\n",
    "\n",
    " # hapus baris-baris yang cocok dengan pola\n",
    " sed '/pattern/d'\n",
    "\n",
    " # hapus SEMUA baris kosong dari sebuah berkas (sama dengan \"grep '.' \")\n",
    " sed '/^$/d'                           # method 1\n",
    " sed '/./!d'                           # method 2\n",
    "\n",
    " # delete all CONSECUTIVE blank lines from file except the first; also\n",
    " # deletes all blank lines from top and end of file (emulates \"cat -s\")\n",
    " sed '/./,/^$/!d'          # method 1, allows 0 blanks at top, 1 at EOF\n",
    " sed '/^$/N;/\\n$/D'        # method 2, allows 1 blank at top, 0 at EOF\n",
    "\n",
    " # delete all CONSECUTIVE blank lines from file except the first 2:\n",
    " sed '/^$/N;/\\n$/N;//D'\n",
    "\n",
    " # delete all leading blank lines at top of file\n",
    " sed '/./,$!d'\n",
    "\n",
    " # delete all trailing blank lines at end of file\n",
    " sed -e :a -e '/^\\n*$/{$d;N;ba' -e '}'  # works on all seds\n",
    " sed -e :a -e '/^\\n*$/N;/\\n$/ba'        # ditto, except for gsed 3.02.*\n",
    "\n",
    " # hapus baris terakhir dari setiap paragraf\n",
    " sed -n '/^$/{p;h;};/./{x;/./p;}'\n",
    "\n",
    "PENERAPAN ISTIMEWA:\n",
    "\n",
    " # remove nroff overstrikes (char, backspace) from man pages. The 'echo'\n",
    " # command may need an -e switch if you use Unix System V or bash shell.\n",
    " sed \"s/.`echo \\\\\\b`//g\"    # double quotes required for Unix environment\n",
    " sed 's/.^H//g'             # in bash/tcsh, press Ctrl-V and then Ctrl-H\n",
    " sed 's/.\\x08//g'           # hex expression for sed 1.5, GNU sed, ssed\n",
    "\n",
    " # get Usenet/e-mail message header\n",
    " sed '/^$/q'                # deletes everything after first blank line\n",
    "\n",
    " # get Usenet/e-mail message body\n",
    " sed '1,/^$/d'              # deletes everything up to first blank line\n",
    "\n",
    " # get Subject header, but remove initial \"Subject: \" portion\n",
    " sed '/^Subject: */!d; s///;q'\n",
    "\n",
    " # get return address header\n",
    " sed '/^Reply-To:/q; /^From:/h; /./d;g;q'\n",
    "\n",
    " # parse out the address proper. Pulls out the e-mail address by itself\n",
    " # from the 1-line return address header (see preceding script)\n",
    " sed 's/ *(.*)//; s/>.*//; s/.*[:<] *//'\n",
    "\n",
    " # add a leading angle bracket and space to each line (quote a message)\n",
    " sed 's/^/> /'\n",
    "\n",
    " # delete leading angle bracket & space from each line (unquote a message)\n",
    " sed 's/^> //'\n",
    "\n",
    " # remove most HTML tags (accommodates multiple-line tags)\n",
    " sed -e :a -e 's/<[^>]*>//g;/</N;//ba'\n",
    "\n",
    " # extract multi-part uuencoded binaries, removing extraneous header\n",
    " # info, so that only the uuencoded portion remains. Files passed to\n",
    " # sed must be passed in the proper order. Version 1 can be entered\n",
    " # from the command line; version 2 can be made into an executable\n",
    " # Unix shell script. (Modified from a script by Rahul Dhesi.)\n",
    " sed '/^end/,/^begin/d' file1 file2 ... fileX | uudecode   # vers. 1\n",
    " sed '/^end/,/^begin/d' \"$@\" | uudecode                    # vers. 2\n",
    "\n",
    " # sort paragraphs of file alphabetically. Paragraphs are separated by blank\n",
    " # lines. GNU sed uses \\v for vertical tab, or any unique char will do.\n",
    " sed '/./{H;d;};x;s/\\n/={NL}=/g' file | sort | sed '1s/={NL}=//;s/={NL}=/\\n/g'\n",
    " gsed '/./{H;d};x;y/\\n/\\v/' file | sort | sed '1s/\\v//;y/\\v/\\n/'\n",
    "\n",
    " # zip up each .TXT file individually, deleting the source file and\n",
    " # setting the name of each .ZIP file to the basename of the .TXT file\n",
    " # (under DOS: the \"dir /b\" switch returns bare filenames in all caps).\n",
    " echo @echo off >zipup.bat\n",
    " dir /b *.txt | sed \"s/^\\(.*\\)\\.TXT/pkzip -mo \\1 \\1.TXT/\" >>zipup.bat\n",
    "\n",
    "TYPICAL USE: Sed takes one or more editing commands and applies all of\n",
    "them, in sequence, to each line of input. After all the commands have\n",
    "been applied to the first input line, that line is output and a second\n",
    "input line is taken for processing, and the cycle repeats. The\n",
    "preceding examples assume that input comes from the standard input\n",
    "device (i.e, the console, normally this will be piped input). One or\n",
    "more filenames can be appended to the command line if the input does\n",
    "not come from stdin. Output is sent to stdout (the screen). Thus:\n",
    "\n",
    " cat filename | sed '10q'        # uses piped input\n",
    " sed '10q' filename              # same effect, avoids a useless \"cat\"\n",
    " sed '10q' filename > newfile    # redirects output to disk\n",
    "\n",
    "For additional syntax instructions, including the way to apply editing\n",
    "commands from a disk file instead of the command line, consult \"sed &\n",
    "awk, 2nd Edition,\" by Dale Dougherty and Arnold Robbins (O'Reilly,\n",
    "1997; http://www.ora.com), \"UNIX Text Processing,\" by Dale Dougherty\n",
    "and Tim O'Reilly (Hayden Books, 1987) or the tutorials by Mike Arst\n",
    "distributed in U-SEDIT2.ZIP (many sites). To fully exploit the power\n",
    "of sed, one must understand \"regular expressions.\" For this, see\n",
    "\"Mastering Regular Expressions\" by Jeffrey Friedl (O'Reilly, 1997).\n",
    "The manual (\"man\") pages on Unix systems may be helpful (try \"man\n",
    "sed\", \"man regexp\", or the subsection on regular expressions in \"man\n",
    "ed\"), but man pages are notoriously difficult. They are not written to\n",
    "teach sed use or regexps to first-time users, but as a reference text\n",
    "for those already acquainted with these tools.\n",
    "\n",
    "QUOTING SYNTAX: The preceding examples use single quotes ('...')\n",
    "instead of double quotes (\"...\") to enclose editing commands, since\n",
    "sed is typically used on a Unix platform. Single quotes prevent the\n",
    "Unix shell from intrepreting the dollar sign ($) and backquotes\n",
    "(`...`), which are expanded by the shell if they are enclosed in\n",
    "double quotes. Users of the \"csh\" shell and derivatives will also need\n",
    "to quote the exclamation mark (!) with the backslash (i.e., \\!) to\n",
    "properly run the examples listed above, even within single quotes.\n",
    "Versions of sed written for DOS invariably require double quotes\n",
    "(\"...\") instead of single quotes to enclose editing commands.\n",
    "\n",
    "USE OF '\\t' IN SED SCRIPTS: For clarity in documentation, we have used\n",
    "the expression '\\t' to indicate a tab character (0x09) in the scripts.\n",
    "However, most versions of sed do not recognize the '\\t' abbreviation,\n",
    "so when typing these scripts from the command line, you should press\n",
    "the TAB key instead. '\\t' is supported as a regular expression\n",
    "metacharacter in awk, perl, and HHsed, sedmod, and GNU sed v3.02.80.\n",
    "\n",
    "VERSIONS OF SED: Versions of sed do differ, and some slight syntax\n",
    "variation is to be expected. In particular, most do not support the\n",
    "use of labels (:name) or branch instructions (b,t) within editing\n",
    "commands, except at the end of those commands. We have used the syntax\n",
    "which will be portable to most users of sed, even though the popular\n",
    "GNU versions of sed allow a more succinct syntax. When the reader sees\n",
    "a fairly long command such as this:\n",
    "\n",
    "   sed -e '/AAA/b' -e '/BBB/b' -e '/CCC/b' -e d\n",
    "\n",
    "it is heartening to know that GNU sed will let you reduce it to:\n",
    "\n",
    "   sed '/AAA/b;/BBB/b;/CCC/b;d'      # or even\n",
    "   sed '/AAA\\|BBB\\|CCC/b;d'\n",
    "\n",
    "In addition, remember that while many versions of sed accept a command\n",
    "like \"/one/ s/RE1/RE2/\", some do NOT allow \"/one/! s/RE1/RE2/\", which\n",
    "contains space before the 's'. Omit the space when typing the command.\n",
    "\n",
    "OPTIMIZING FOR SPEED: If execution speed needs to be increased (due to\n",
    "large input files or slow processors or hard disks), substitution will\n",
    "be executed more quickly if the \"find\" expression is specified before\n",
    "giving the \"s/.../.../\" instruction. Thus:\n",
    "\n",
    "   sed 's/foo/bar/g' filename         # standard replace command\n",
    "   sed '/foo/ s/foo/bar/g' filename   # executes more quickly\n",
    "   sed '/foo/ s//bar/g' filename      # shorthand sed syntax\n",
    "\n",
    "On line selection or deletion in which you only need to output lines\n",
    "from the first part of the file, a \"quit\" command (q) in the script\n",
    "will drastically reduce processing time for large files. Thus:\n",
    "\n",
    "   sed -n '45,50p' filename           # print line nos. 45-50 of a file\n",
    "   sed -n '51q;45,50p' filename       # same, but executes much faster\n",
    "\n",
    "If you have any additional scripts to contribute or if you find errors\n",
    "in this document, please send e-mail to the compiler. Indicate the\n",
    "version of sed you used, the operating system it was compiled for, and\n",
    "the nature of the problem. To qualify as a one-liner, the command line\n",
    "must be 65 characters or less. Various scripts in this file have been\n",
    "written or contributed by:\n",
    "\n",
    " Al Aab                   # founder of \"seders\" list\n",
    " Edgar Allen              # various\n",
    " Yiorgos Adamopoulos      # various\n",
    " Dale Dougherty           # author of \"sed & awk\"\n",
    " Carlos Duarte            # author of \"do it with sed\"\n",
    " Eric Pement              # author of this document\n",
    " Ken Pizzini              # author of GNU sed v3.02\n",
    " S.G. Ravenhall           # great de-html script\n",
    " Greg Ubben               # many contributions & much help\n",
    "-------------------------------------------------------------------------\n",
    "perl -ne 'if ( /^>/ ) {print \"\\n\";print;} else {chomp; print;}' $1\n",
    "\n",
    "                                                         \n",
    "                                                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Print only line number 101 in file:\n",
    "awk 'NR==101' file\n",
    "Print lines 101-202 in file:\n",
    "awk 'NR==101, NR==202' file\n",
    "Print every 4th line in a file:\n",
    "awk '0 == NR % 4' file\n",
    "Print header with column/field numbers in a tab-separated file:\n",
    "awk -F \"\\t\" '{ for (f=1; f<=NF; f++) print f\":\"$f; exit }' file\n",
    "Rename multiple files (/bin/sh executes):\n",
    "ls * | awk '{ print \"mv \"$1\" \"$1 }' | sed 's/to_replace/replace_with/2' | /bin/sh\n",
    "Remove duplicate, nonconsecutive lines (no need to sort first):\n",
    "awk '!a[$0]++' file\n",
    "Put every other line next to the one above, separated by tab:\n",
    "awk 'ORS=NR%2 ? \"\\t\" : \"\\n\"' file\n",
    "Print section of file between two regexes:\n",
    "awk '/regex1/, /regex2/' file\n",
    "Print average of third column:\n",
    "awk '{ sum += $3 } END { if (NR > 0) print sum / NR }' file\n",
    "Sort on 3rd column, then print fields 2 through 5 of a tabular file, and align output:\n",
    "sort -nk 3 file | cut -f 2-5 | column -t\n",
    "Print number of files present in each subdirectory of the current directory:\n",
    "find . -maxdepth 1 -type d -exec bash -c \"echo -ne '{} '; ls '{}' | wc -l\" \\;\n",
    "Check if the same file exists in all directories:\n",
    "for d in ./bin*; do [[ -f ./$d/RAxML_bootstrap-out ]] && echo \"File exist in dir $d\" || echo \"File does not exist in dir $d\"; done\n",
    "Find directories that are missing files with the word masked in their name:\n",
    "find base_dir -type d '!' -exec sh -c 'ls -1 \"{}\" | grep -q \"*masked*\"' ';' -print\n",
    "\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
